{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.059875,
     "end_time": "2020-12-15T06:52:44.191791",
     "exception": false,
     "start_time": "2020-12-15T06:52:44.131916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ‚òòÔ∏è PLANT DISEASE CLASSIFICATION USING Custom CNN ‚òòÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056728,
     "end_time": "2020-12-15T06:52:44.447613",
     "exception": false,
     "start_time": "2020-12-15T06:52:44.390885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Description of the dataset üìù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057571,
     "end_time": "2020-12-15T06:52:44.675922",
     "exception": false,
     "start_time": "2020-12-15T06:52:44.618351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Our goal üéØ\n",
    "Goal is clear and simple. We need to build a model, which can classify between healthy and diseased crop leaves and also if the crop have any disease, predict which disease is it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056957,
     "end_time": "2020-12-15T06:52:44.789985",
     "exception": false,
     "start_time": "2020-12-15T06:52:44.733028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Let's get started...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056658,
     "end_time": "2020-12-15T06:52:44.903338",
     "exception": false,
     "start_time": "2020-12-15T06:52:44.846680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057862,
     "end_time": "2020-12-15T06:52:45.017851",
     "exception": false,
     "start_time": "2020-12-15T06:52:44.959989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's import required modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058411,
     "end_time": "2020-12-15T06:52:54.403660",
     "exception": false,
     "start_time": "2020-12-15T06:52:54.345249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We would require torchsummary library to print the model's summary in keras style (nicely formatted and pretty to look) as Pytorch natively doesn't support that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthatiayyappaswami\u001b[0m (\u001b[33mthatiayyappaswami-sam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:54.531023Z",
     "iopub.status.busy": "2020-12-15T06:52:54.530191Z",
     "iopub.status.idle": "2020-12-15T06:52:56.166219Z",
     "shell.execute_reply": "2020-12-15T06:52:56.164951Z"
    },
    "papermill": {
     "duration": 1.704433,
     "end_time": "2020-12-15T06:52:56.166377",
     "exception": false,
     "start_time": "2020-12-15T06:52:54.461944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import random\n",
    "import platform\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "# This is a Jupyter magic command to display plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058296,
     "end_time": "2020-12-15T06:52:56.283998",
     "exception": false,
     "start_time": "2020-12-15T06:52:56.225702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üß≠ Exploring the data üß≠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.05825,
     "end_time": "2020-12-15T06:52:56.400725",
     "exception": false,
     "start_time": "2020-12-15T06:52:56.342475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:56.522416Z",
     "iopub.status.busy": "2020-12-15T06:52:56.521802Z",
     "iopub.status.idle": "2020-12-15T06:52:56.536807Z",
     "shell.execute_reply": "2020-12-15T06:52:56.536213Z"
    },
    "papermill": {
     "duration": 0.07813,
     "end_time": "2020-12-15T06:52:56.536899",
     "exception": false,
     "start_time": "2020-12-15T06:52:56.458769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./new_data\"\n",
    "train_dir = DATA_DIR + \"/train\"\n",
    "valid_dir = DATA_DIR + \"/val\"\n",
    "test_dir = DATA_DIR + \"/test\"\n",
    "diseases = os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:56.660806Z",
     "iopub.status.busy": "2020-12-15T06:52:56.660131Z",
     "iopub.status.idle": "2020-12-15T06:52:56.663554Z",
     "shell.execute_reply": "2020-12-15T06:52:56.664320Z"
    },
    "papermill": {
     "duration": 0.068176,
     "end_time": "2020-12-15T06:52:56.664465",
     "exception": false,
     "start_time": "2020-12-15T06:52:56.596289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rice___tungro', 'Soybean__bacterial_blight', 'Tea__bird_eye_spot', 'Bell_pepper___bacterial_spot', 'Sugarcane__bacterial_blight', 'Apple___scab', 'Cassava___green_mottle', 'Jamun__healthy', 'Apple___healthy', 'Soybean__diabrotica_speciosa', 'Pomegranate__diseased', 'Rose___healthy', 'Tomato___late_blight', 'Tomato__yellow_leaf_curl_virus', 'Potato___early_blight', 'Chili__leaf curl', 'Wheat__septoria', 'Tea__brown_blight', 'Rice___bacterial_blight', 'Sugarcane__red_stripe', 'Potato___healthy', 'Pepper_bell__healthy', 'Tomato___mosaic_virus', 'Soybean__mosaic_virus', 'Coffee___rust', 'Coffee__cercospora_leaf_spot', 'Gauva__diseased', 'Mango__healthy', 'Strawberry___healthy', 'Potato___phytophthora', 'Wheat__brown_rust', 'Tomato___leaf_mold', 'Wheat__healthy', 'Rose___rust', 'Bell_pepper___healthy', 'Soybean__southern_blight', 'Peach___healthy', 'Cherry___powdery_mildew', 'Grape___black_measles', 'Tomato___leaf_curl', 'Potato___nematode', 'Pepper_bell__bacterial_spot', 'Rose___slug_sawfly', 'Tea__algal_leaf', 'Grape___healthy', 'Grape___Leaf_blight', 'Rice___blast', 'Lemon__healthy', 'Apple___black_rot', 'Tomato___healthy', 'Corn___northern_leaf_blight', 'Cassava___bacterial_blight', 'Soybean__powdery_mildew', 'Coffee___red_spider_mite', 'Rice__hispa', 'Apple___brown_spot', 'Potato___pests', 'Soybean__rust', 'Sugercane___yellow_leaf', 'Tea__anthracnose', 'Cherry___healthy', 'Blueberry___healthy', 'Sugercane___healthy', 'Potato___bacterial_wilt', 'Sugercane___rust', 'Peach___bacterial_spot', 'Lemon__diseased', 'Sugercane___mosaic', 'Cassava___healthy', 'Tomato___early_blight', 'Watermelon___mosaic_virus', 'Watermelon___anthracnose', 'Cassava___mosaic_disease', 'Mango__diseased', 'Sugercane___red_rot', 'Cassava___brown_streak_disease', 'Chili__whitefly', 'Potato___mosaic_virus', 'Chili__healthy', 'Tomato___spider_mites', 'Soybean__healthy', 'Tomato___septoria_leaf_spot', 'Wheat__yellow_rust', 'Soybean__caterpillar', 'Potato___leafroll_virus', 'Squash___powdery_mildew', 'Rice__leaf_blast', 'Rice__healthy', 'Jamun__diseased', 'Apple___alternaria_leaf_spot', 'Grape___black_rot', 'Orange___citrus_greening', 'Strawberry___leaf_scorch', 'Watermelon___healthy', 'Raspberry___healthy', 'Coffee___healthy', 'Pomegranate__healthy', 'Apple___rust', 'Tomato___bacterial_spot', 'Rice___brown_spot', 'Cucumber__diseased', 'Tea__healthy', 'Corn__gray_leaf_spot', 'Chili__leaf spot', 'Corn___healthy', 'Potato___late_blight', 'Tomato___target_spot', 'Soybean__downy_mildew', 'Cucumber__healthy', 'Watermelon___downy_mildew', 'Rice__neck_blast', 'Chili__yellowish', 'Gauva__healthy', 'Apple___gray_spot', 'Corn___common_rust']\n"
     ]
    }
   ],
   "source": [
    "# printing the disease names\n",
    "print(diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:56.789812Z",
     "iopub.status.busy": "2020-12-15T06:52:56.789079Z",
     "iopub.status.idle": "2020-12-15T06:52:56.792917Z",
     "shell.execute_reply": "2020-12-15T06:52:56.792464Z"
    },
    "papermill": {
     "duration": 0.068791,
     "end_time": "2020-12-15T06:52:56.793019",
     "exception": false,
     "start_time": "2020-12-15T06:52:56.724228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total disease classes are: 115\n"
     ]
    }
   ],
   "source": [
    "print(\"Total disease classes are: {}\".format(len(diseases)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"SOTA_CustomCNN\"\n",
    "\n",
    "\n",
    "# increase this as classes increases\n",
    "NUM_CLASSES = 115\n",
    "\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "CHECKPOINT_DIR = f\"./models/{MODEL_NAME}\"\n",
    "RESULTS_DIR = f\"./results/{MODEL_NAME}\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "WANDB_PROJECT_NAME = \"leafy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_BATCH_SIZE = 16 \n",
    "\n",
    "# The desired effective batch size for stable gradients\n",
    "VIRTUAL_BATCH_SIZE = 64 \n",
    "\n",
    "# Calculate the number of accumulation steps needed\n",
    "# This ensures VIRTUAL_BATCH_SIZE is a multiple of REAL_BATCH_SIZE\n",
    "assert VIRTUAL_BATCH_SIZE % REAL_BATCH_SIZE == 0, \"Virtual batch size must be a multiple of real batch size!\"\n",
    "accumulation_steps = VIRTUAL_BATCH_SIZE // REAL_BATCH_SIZE\n",
    "\n",
    "# Set the dataloader's batch size to the REAL batch size\n",
    "# BATCH_SIZE = REAL_BATCH_SIZE\n",
    "\n",
    "\n",
    "BATCH_SIZE_PHASE1 = 192\n",
    "BATCH_SIZE_PHASE2 = 96\n",
    "\n",
    "\n",
    "IMAGE_SIZE_PHASE1 = 192\n",
    "IMAGE_SIZE_PHASE2 = 256\n",
    "\n",
    "NUM_EPOCHS_PHASE1 = 20 \n",
    "NUM_EPOCHS_PHASE2 = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes set to: 115 (Type: <class 'int'>)\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Sanity Check ---\n",
    "print(f\"Number of classes set to: {NUM_CLASSES} (Type: {type(NUM_CLASSES)})\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(f'runs/{MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.069462,
     "end_time": "2020-12-15T06:52:57.055273",
     "exception": false,
     "start_time": "2020-12-15T06:52:56.985811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The above cell extract the number of unique plants and number of unique diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plants = []\n",
    "NumberOfDiseases = 0\n",
    "for plant in diseases:\n",
    "    if plant.split('__')[0] not in plants:\n",
    "        plants.append(plant.split('__')[0])\n",
    "    if plant.split('__')[1] != 'healthy':\n",
    "        NumberOfDiseases += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:57.183047Z",
     "iopub.status.busy": "2020-12-15T06:52:57.182397Z",
     "iopub.status.idle": "2020-12-15T06:52:57.186314Z",
     "shell.execute_reply": "2020-12-15T06:52:57.185696Z"
    },
    "papermill": {
     "duration": 0.068933,
     "end_time": "2020-12-15T06:52:57.186415",
     "exception": false,
     "start_time": "2020-12-15T06:52:57.117482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Plants are: \n",
      "['Rice', 'Soybean', 'Tea', 'Bell_pepper', 'Sugarcane', 'Apple', 'Cassava', 'Jamun', 'Pomegranate', 'Rose', 'Tomato', 'Potato', 'Chili', 'Wheat', 'Pepper_bell', 'Coffee', 'Gauva', 'Mango', 'Strawberry', 'Peach', 'Cherry', 'Grape', 'Lemon', 'Corn', 'Sugercane', 'Blueberry', 'Watermelon', 'Squash', 'Orange', 'Raspberry', 'Cucumber']\n"
     ]
    }
   ],
   "source": [
    "# unique plants in the dataset\n",
    "print(f\"Unique Plants are: \\n{plants}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:57.313744Z",
     "iopub.status.busy": "2020-12-15T06:52:57.313076Z",
     "iopub.status.idle": "2020-12-15T06:52:57.316355Z",
     "shell.execute_reply": "2020-12-15T06:52:57.317088Z"
    },
    "papermill": {
     "duration": 0.069891,
     "end_time": "2020-12-15T06:52:57.317251",
     "exception": false,
     "start_time": "2020-12-15T06:52:57.247360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of plants: 31\n"
     ]
    }
   ],
   "source": [
    "# number of unique plants\n",
    "print(\"Number of plants: {}\".format(len(plants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:57.444856Z",
     "iopub.status.busy": "2020-12-15T06:52:57.444130Z",
     "iopub.status.idle": "2020-12-15T06:52:57.447598Z",
     "shell.execute_reply": "2020-12-15T06:52:57.448386Z"
    },
    "papermill": {
     "duration": 0.069339,
     "end_time": "2020-12-15T06:52:57.448580",
     "exception": false,
     "start_time": "2020-12-15T06:52:57.379241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of diseases: 103\n"
     ]
    }
   ],
   "source": [
    "# number of unique diseases\n",
    "print(\"Number of diseases: {}\".format(NumberOfDiseases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = f\"./models/{MODEL_NAME}\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.062489,
     "end_time": "2020-12-15T06:52:57.574134",
     "exception": false,
     "start_time": "2020-12-15T06:52:57.511645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So we have images of leaves of 14 plants and while excluding healthy leaves, we have 26 types of images that show a particular disease in a particular plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:57.704194Z",
     "iopub.status.busy": "2020-12-15T06:52:57.703583Z",
     "iopub.status.idle": "2020-12-15T06:53:03.960106Z",
     "shell.execute_reply": "2020-12-15T06:53:03.960800Z"
    },
    "papermill": {
     "duration": 6.323955,
     "end_time": "2020-12-15T06:53:03.960987",
     "exception": false,
     "start_time": "2020-12-15T06:52:57.637032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no. of images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rice___tungro</th>\n",
       "      <td>1046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soybean__bacterial_blight</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tea__bird_eye_spot</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bell_pepper___bacterial_spot</th>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sugarcane__bacterial_blight</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rice__neck_blast</th>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chili__yellowish</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gauva__healthy</th>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple___gray_spot</th>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Corn___common_rust</th>\n",
       "      <td>953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              no. of images\n",
       "Rice___tungro                          1046\n",
       "Soybean__bacterial_blight                44\n",
       "Tea__bird_eye_spot                      240\n",
       "Bell_pepper___bacterial_spot            797\n",
       "Sugarcane__bacterial_blight              80\n",
       "...                                     ...\n",
       "Rice__neck_blast                        800\n",
       "Chili__yellowish                         80\n",
       "Gauva__healthy                          221\n",
       "Apple___gray_spot                       316\n",
       "Corn___common_rust                      953\n",
       "\n",
       "[115 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of images for each disease\n",
    "nums = {}\n",
    "for disease in diseases:\n",
    "    nums[disease] = len(os.listdir(train_dir + '/' + disease))\n",
    "    \n",
    "# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\n",
    "\n",
    "img_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=[\"no. of images\"])\n",
    "img_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.090681,
     "end_time": "2020-12-15T06:53:04.148485",
     "exception": false,
     "start_time": "2020-12-15T06:53:04.057804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Visualizing the above information on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:53:04.334832Z",
     "iopub.status.busy": "2020-12-15T06:53:04.333992Z",
     "iopub.status.idle": "2020-12-15T06:53:04.860737Z",
     "shell.execute_reply": "2020-12-15T06:53:04.859673Z"
    },
    "papermill": {
     "duration": 0.623297,
     "end_time": "2020-12-15T06:53:04.860887",
     "exception": false,
     "start_time": "2020-12-15T06:53:04.237590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Images per each class of plant disease')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plotting number of images available for each disease\n",
    "index = [n for n in range(NUM_CLASSES)]\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(index, [n for n in nums.values()], width=0.3)\n",
    "plt.xlabel('Plants/Diseases', fontsize=10)\n",
    "plt.ylabel('No of images available', fontsize=10)\n",
    "plt.xticks(index, diseases, fontsize=5, rotation=90)\n",
    "plt.title('Images per each class of plant disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.068183,
     "end_time": "2020-12-15T06:53:04.997832",
     "exception": false,
     "start_time": "2020-12-15T06:53:04.929649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can see that the dataset is almost balanced for all classes, so we are good to go forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.073886,
     "end_time": "2020-12-15T06:53:05.152902",
     "exception": false,
     "start_time": "2020-12-15T06:53:05.079016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Images available for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:53:05.285106Z",
     "iopub.status.busy": "2020-12-15T06:53:05.284437Z",
     "iopub.status.idle": "2020-12-15T06:53:05.287876Z",
     "shell.execute_reply": "2020-12-15T06:53:05.288406Z"
    },
    "papermill": {
     "duration": 0.07225,
     "end_time": "2020-12-15T06:53:05.288530",
     "exception": false,
     "start_time": "2020-12-15T06:53:05.216280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 113451 images for training\n"
     ]
    }
   ],
   "source": [
    "n_train = 0\n",
    "for value in nums.values():\n",
    "    n_train += value\n",
    "print(f\"There are {n_train} images for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch Version: 2.8.0+cu128\n",
      "CUDA Version: 12.8\n",
      "GPU: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.064118,
     "end_time": "2020-12-15T06:53:05.416780",
     "exception": false,
     "start_time": "2020-12-15T06:53:05.352662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üç≥ Data Preparation for training üç≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:53:05.548188Z",
     "iopub.status.busy": "2020-12-15T06:53:05.547365Z",
     "iopub.status.idle": "2020-12-15T06:54:23.286526Z",
     "shell.execute_reply": "2020-12-15T06:54:23.285629Z"
    },
    "papermill": {
     "duration": 77.805914,
     "end_time": "2020-12-15T06:54:23.286647",
     "exception": false,
     "start_time": "2020-12-15T06:53:05.480733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data with STRONGER augmentations...\n",
      "Dataset sizes: Train=113451, Val=14118\n"
     ]
    }
   ],
   "source": [
    "data_transforms_phase1 = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(IMAGE_SIZE_PHASE1),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.TrivialAugmentWide(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE_PHASE1 + 32),\n",
    "        transforms.CenterCrop(IMAGE_SIZE_PHASE1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "print(\"Loading data with STRONGER augmentations...\")\n",
    "image_datasets_phase1 = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x), data_transforms_phase1[x]) for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes_phase1 = {x: len(image_datasets_phase1[x]) for x in ['train', 'val']}\n",
    "class_names_phase1 = image_datasets_phase1['train'].classes\n",
    "print(f\"Dataset sizes: Train={dataset_sizes_phase1['train']}, Val={dataset_sizes_phase1['val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 DataLoaders created. Class names saved.\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(RESULTS_DIR, 'class_names.json'), 'w') as f:\n",
    "    json.dump(class_names_phase1, f)\n",
    "print(f\"Phase 1 DataLoaders created. Class names saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A \"Good and Complex\" Custom CNN Model Definition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attempting Model Compilation ---\n",
      "Linux OS detected. Attempting to compile the model...\n",
      "Model compiled successfully!\n",
      "--- Model Summary for SOTA_CustomCNN (before compilation) ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 161\u001b[0m\n\u001b[1;32m    159\u001b[0m summary_str \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mStringIO()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mredirect_stdout(summary_str):\n\u001b[0;32m--> 161\u001b[0m     summary(model, input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, IMAGE_SIZE_PHASE2, IMAGE_SIZE_PHASE2), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Use CPU for summary\u001b[39;00m\n\u001b[1;32m    162\u001b[0m model_summary \u001b[38;5;241m=\u001b[39m summary_str\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(RESULTS_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_summary.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model(\u001b[38;5;241m*\u001b[39mx)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:375\u001b[0m, in \u001b[0;36mOptimizedModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39m_has_any_global_hook():\n\u001b[1;32m    366\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `torch.compile(module)` when there are global hooks on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodules (e.g., from `register_module_forward_hook`); this will\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    374\u001b[0m     )\n\u001b[0;32m--> 375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:736\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    733\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1879\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1878\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1881\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1882\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1827\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1824\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1825\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1827\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1829\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1830\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1832\u001b[0m     ):\n\u001b[1;32m   1833\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1495\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1489\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1490\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1491\u001b[0m             )\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_torchdynamo_orig_callable(\n\u001b[1;32m   1496\u001b[0m         frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state, skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1497\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1272\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m   1270\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1272\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_convert(\n\u001b[1;32m   1273\u001b[0m         frame, cache_entry, hooks, frame_state, skip\u001b[38;5;241m=\u001b[39mskip \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1274\u001b[0m     )\n\u001b[1;32m   1275\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:629\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    626\u001b[0m     dynamo_tls\u001b[38;5;241m.\u001b[39mtraced_frame_infos\u001b[38;5;241m.\u001b[39mappend(info)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(\n\u001b[1;32m    630\u001b[0m         frame\u001b[38;5;241m.\u001b[39mf_code,\n\u001b[1;32m    631\u001b[0m         frame\u001b[38;5;241m.\u001b[39mf_globals,\n\u001b[1;32m    632\u001b[0m         frame\u001b[38;5;241m.\u001b[39mf_locals,\n\u001b[1;32m    633\u001b[0m         frame\u001b[38;5;241m.\u001b[39mf_builtins,\n\u001b[1;32m    634\u001b[0m         frame\u001b[38;5;241m.\u001b[39mclosure,\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_torchdynamo_orig_callable,\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_one_graph,\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export,\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_constraints,\n\u001b[1;32m    639\u001b[0m         hooks,\n\u001b[1;32m    640\u001b[0m         cache_entry,\n\u001b[1;32m    641\u001b[0m         cache_size,\n\u001b[1;32m    642\u001b[0m         frame,\n\u001b[1;32m    643\u001b[0m         frame_state\u001b[38;5;241m=\u001b[39mframe_state,\n\u001b[1;32m    644\u001b[0m         compile_id\u001b[38;5;241m=\u001b[39mcompile_id,\n\u001b[1;32m    645\u001b[0m         skip\u001b[38;5;241m=\u001b[39mskip \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    646\u001b[0m         package\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_package,\n\u001b[1;32m    647\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1111\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip, package)\u001b[0m\n\u001b[1;32m   1109\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1111\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m compile_inner(code, one_graph, hooks, transform)\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# NB: We only put_code_state in success case.  Success case here\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# does include graph breaks; specifically, if a graph break still\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# resulted in a partially compiled graph, we WILL return here.  An\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# to upload for graph break though, because this can prevent\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# extra graph break compilations.)\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     put_code_state()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_utils_internal.py:97\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# This is not needed but we have it here to avoid having profile_compile_time\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# in stack traces when profiling is not enabled.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m    100\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    101\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:793\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    787\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m    788\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39minstall_callbacks(\n\u001b[1;32m    789\u001b[0m             CallbackTrigger\u001b[38;5;241m.\u001b[39mDYNAMO, \u001b[38;5;28mstr\u001b[39m(CompileContext\u001b[38;5;241m.\u001b[39mcurrent_compile_id())\n\u001b[1;32m    790\u001b[0m         )\n\u001b[1;32m    791\u001b[0m     )\n\u001b[1;32m    792\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord())\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile_inner(code, one_graph, hooks, transform)\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    796\u001b[0m     ConvertFrameReturn()\n\u001b[1;32m    797\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:832\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m    830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_attempt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    831\u001b[0m     ):\n\u001b[0;32m--> 832\u001b[0m         out_code \u001b[38;5;241m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py:1424\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1421\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1422\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1424\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:267\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(torch_function_mode_stack_state_mgr)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:753\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    751\u001b[0m     tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mmark_bytecode_tracing_start()\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 753\u001b[0m         tracer\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    755\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3497\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 3497\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1363\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer\n\u001b[0;32m-> 1363\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep():\n\u001b[1;32m   1364\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1267\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_table[inst\u001b[38;5;241m.\u001b[39mopcode](\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:834\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_fn(\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_generic_context_managers:\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2910\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 2910\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inst)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2904\u001b[0m, in \u001b[0;36mInstructionTranslatorBase._call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2902\u001b[0m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[1;32m   2903\u001b[0m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[0;32m-> 2904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function(fn, args, kwargs)\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2906\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkw_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1193\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(fn\u001b[38;5;241m.\u001b[39mcall_function(\u001b[38;5;28mself\u001b[39m, args, kwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/lazy.py:201\u001b[0m, in \u001b[0;36m_create_realize_and_forward.<locals>.realize_and_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mgetattr\u001b[39m(VariableTracker, name))\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrealize_and_forward\u001b[39m(\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m: LazyVariableTracker, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrealize(), name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py:1000\u001b[0m, in \u001b[0;36mUnspecializedNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m ctx \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    993\u001b[0m     record_nn_module_stack(\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(mod)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_nn_module_stack_source(), tx, mod\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m nullcontext()\n\u001b[1;32m    998\u001b[0m )\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m-> 1000\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mUserFunctionVariable(fn, source\u001b[38;5;241m=\u001b[39msource)\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m   1001\u001b[0m         tx, [\u001b[38;5;28mself\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(args), kwargs\n\u001b[1;32m   1002\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:529\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mside_effects\u001b[38;5;241m.\u001b[39mallow_side_effects_under_checkpoint(tx):\n\u001b[1;32m    528\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n\u001b[0;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:293\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    289\u001b[0m     tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    290\u001b[0m     args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    291\u001b[0m     kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    292\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tx\u001b[38;5;241m.\u001b[39minline_user_function_return(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_args(), \u001b[38;5;241m*\u001b[39margs], kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1210\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minline_generator_function(fn, args, kwargs)\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InliningInstructionTranslator\u001b[38;5;241m.\u001b[39minline_call(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3698\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   3696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[1;32m   3697\u001b[0m     tracer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_inline_tracer(parent, func, args, kwargs)\n\u001b[0;32m-> 3698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minline_call_()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3901\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3899\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3900\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 3901\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3903\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1363\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer\n\u001b[0;32m-> 1363\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep():\n\u001b[1;32m   1364\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1267\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_table[inst\u001b[38;5;241m.\u001b[39mopcode](\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:834\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_fn(\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_generic_context_managers:\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2910\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 2910\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inst)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2904\u001b[0m, in \u001b[0;36mInstructionTranslatorBase._call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2902\u001b[0m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[1;32m   2903\u001b[0m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[0;32m-> 2904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function(fn, args, kwargs)\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2906\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkw_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1193\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(fn\u001b[38;5;241m.\u001b[39mcall_function(\u001b[38;5;28mself\u001b[39m, args, kwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py:1000\u001b[0m, in \u001b[0;36mUnspecializedNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m ctx \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    993\u001b[0m     record_nn_module_stack(\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(mod)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_nn_module_stack_source(), tx, mod\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m nullcontext()\n\u001b[1;32m    998\u001b[0m )\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m-> 1000\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mUserFunctionVariable(fn, source\u001b[38;5;241m=\u001b[39msource)\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m   1001\u001b[0m         tx, [\u001b[38;5;28mself\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(args), kwargs\n\u001b[1;32m   1002\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:529\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mside_effects\u001b[38;5;241m.\u001b[39mallow_side_effects_under_checkpoint(tx):\n\u001b[1;32m    528\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n\u001b[0;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:293\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    289\u001b[0m     tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    290\u001b[0m     args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    291\u001b[0m     kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    292\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tx\u001b[38;5;241m.\u001b[39minline_user_function_return(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_args(), \u001b[38;5;241m*\u001b[39margs], kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1210\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minline_generator_function(fn, args, kwargs)\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InliningInstructionTranslator\u001b[38;5;241m.\u001b[39minline_call(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3698\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   3696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[1;32m   3697\u001b[0m     tracer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_inline_tracer(parent, func, args, kwargs)\n\u001b[0;32m-> 3698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minline_call_()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3901\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3899\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3900\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 3901\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3903\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1363\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer\n\u001b[0;32m-> 1363\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep():\n\u001b[1;32m   1364\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1267\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_table[inst\u001b[38;5;241m.\u001b[39mopcode](\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:834\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_fn(\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_generic_context_managers:\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2910\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 2910\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inst)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2904\u001b[0m, in \u001b[0;36mInstructionTranslatorBase._call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2902\u001b[0m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[1;32m   2903\u001b[0m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[0;32m-> 2904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function(fn, args, kwargs)\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2906\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkw_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1193\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(fn\u001b[38;5;241m.\u001b[39mcall_function(\u001b[38;5;28mself\u001b[39m, args, kwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:293\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    289\u001b[0m     tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    290\u001b[0m     args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    291\u001b[0m     kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    292\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tx\u001b[38;5;241m.\u001b[39minline_user_function_return(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_args(), \u001b[38;5;241m*\u001b[39margs], kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1210\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minline_generator_function(fn, args, kwargs)\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InliningInstructionTranslator\u001b[38;5;241m.\u001b[39minline_call(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3698\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   3696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[1;32m   3697\u001b[0m     tracer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_inline_tracer(parent, func, args, kwargs)\n\u001b[0;32m-> 3698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minline_call_()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3901\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3899\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3900\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 3901\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3903\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1363\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer\n\u001b[0;32m-> 1363\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep():\n\u001b[1;32m   1364\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1267\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_table[inst\u001b[38;5;241m.\u001b[39mopcode](\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:834\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_fn(\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_generic_context_managers:\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2228\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_EX\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2226\u001b[0m \u001b[38;5;66;03m# Map to a dictionary of str -> VariableTracker\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m kwargsvars \u001b[38;5;241m=\u001b[39m kwargsvars\u001b[38;5;241m.\u001b[39mkeys_as_python_constant()\n\u001b[0;32m-> 2228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function(fn, argsvars\u001b[38;5;241m.\u001b[39mitems, kwargsvars)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1193\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(fn\u001b[38;5;241m.\u001b[39mcall_function(\u001b[38;5;28mself\u001b[39m, args, kwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:1055\u001b[0m, in \u001b[0;36mUserMethodVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mvalue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(tx, fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs)\n\u001b[0;32m-> 1055\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:529\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mside_effects\u001b[38;5;241m.\u001b[39mallow_side_effects_under_checkpoint(tx):\n\u001b[1;32m    528\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n\u001b[0;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:293\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    289\u001b[0m     tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    290\u001b[0m     args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    291\u001b[0m     kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    292\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tx\u001b[38;5;241m.\u001b[39minline_user_function_return(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_args(), \u001b[38;5;241m*\u001b[39margs], kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1210\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minline_generator_function(fn, args, kwargs)\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InliningInstructionTranslator\u001b[38;5;241m.\u001b[39minline_call(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3698\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   3696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[1;32m   3697\u001b[0m     tracer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_inline_tracer(parent, func, args, kwargs)\n\u001b[0;32m-> 3698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minline_call_()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3901\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3899\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3900\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 3901\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3903\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1363\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer\n\u001b[0;32m-> 1363\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep():\n\u001b[1;32m   1364\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1267\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_table[inst\u001b[38;5;241m.\u001b[39mopcode](\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:834\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_fn(\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_generic_context_managers:\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2910\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 2910\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inst)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2904\u001b[0m, in \u001b[0;36mInstructionTranslatorBase._call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2902\u001b[0m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[1;32m   2903\u001b[0m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[0;32m-> 2904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function(fn, args, kwargs)\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2906\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkw_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1193\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(fn\u001b[38;5;241m.\u001b[39mcall_function(\u001b[38;5;28mself\u001b[39m, args, kwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/lazy.py:201\u001b[0m, in \u001b[0;36m_create_realize_and_forward.<locals>.realize_and_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mgetattr\u001b[39m(VariableTracker, name))\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrealize_and_forward\u001b[39m(\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m: LazyVariableTracker, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrealize(), name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py:1000\u001b[0m, in \u001b[0;36mUnspecializedNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m ctx \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    993\u001b[0m     record_nn_module_stack(\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(mod)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_nn_module_stack_source(), tx, mod\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m nullcontext()\n\u001b[1;32m    998\u001b[0m )\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m-> 1000\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mUserFunctionVariable(fn, source\u001b[38;5;241m=\u001b[39msource)\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m   1001\u001b[0m         tx, [\u001b[38;5;28mself\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(args), kwargs\n\u001b[1;32m   1002\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:529\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mside_effects\u001b[38;5;241m.\u001b[39mallow_side_effects_under_checkpoint(tx):\n\u001b[1;32m    528\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n\u001b[0;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:293\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    289\u001b[0m     tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    290\u001b[0m     args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    291\u001b[0m     kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    292\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tx\u001b[38;5;241m.\u001b[39minline_user_function_return(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_args(), \u001b[38;5;241m*\u001b[39margs], kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1210\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minline_generator_function(fn, args, kwargs)\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InliningInstructionTranslator\u001b[38;5;241m.\u001b[39minline_call(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3698\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   3696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[1;32m   3697\u001b[0m     tracer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_inline_tracer(parent, func, args, kwargs)\n\u001b[0;32m-> 3698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minline_call_()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3901\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3899\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3900\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 3901\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3903\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1363\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer\n\u001b[0;32m-> 1363\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep():\n\u001b[1;32m   1364\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1267\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_table[inst\u001b[38;5;241m.\u001b[39mopcode](\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:834\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_fn(\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_generic_context_managers:\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2910\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 2910\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inst)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2904\u001b[0m, in \u001b[0;36mInstructionTranslatorBase._call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2902\u001b[0m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[1;32m   2903\u001b[0m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[0;32m-> 2904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function(fn, args, kwargs)\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2906\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkw_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1193\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(fn\u001b[38;5;241m.\u001b[39mcall_function(\u001b[38;5;28mself\u001b[39m, args, kwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py:1000\u001b[0m, in \u001b[0;36mUnspecializedNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m ctx \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    993\u001b[0m     record_nn_module_stack(\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(mod)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_nn_module_stack_source(), tx, mod\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m nullcontext()\n\u001b[1;32m    998\u001b[0m )\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m-> 1000\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mUserFunctionVariable(fn, source\u001b[38;5;241m=\u001b[39msource)\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m   1001\u001b[0m         tx, [\u001b[38;5;28mself\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(args), kwargs\n\u001b[1;32m   1002\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:529\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mside_effects\u001b[38;5;241m.\u001b[39mallow_side_effects_under_checkpoint(tx):\n\u001b[1;32m    528\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n\u001b[0;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:293\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    289\u001b[0m     tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    290\u001b[0m     args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    291\u001b[0m     kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    292\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tx\u001b[38;5;241m.\u001b[39minline_user_function_return(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_args(), \u001b[38;5;241m*\u001b[39margs], kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1210\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minline_generator_function(fn, args, kwargs)\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InliningInstructionTranslator\u001b[38;5;241m.\u001b[39minline_call(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3698\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   3696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[1;32m   3697\u001b[0m     tracer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_inline_tracer(parent, func, args, kwargs)\n\u001b[0;32m-> 3698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minline_call_()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3901\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3899\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3900\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 3901\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3903\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1363\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer\n\u001b[0;32m-> 1363\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep():\n\u001b[1;32m   1364\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1267\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_table[inst\u001b[38;5;241m.\u001b[39mopcode](\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:834\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_fn(\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_generic_context_managers:\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2910\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 2910\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inst)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2904\u001b[0m, in \u001b[0;36mInstructionTranslatorBase._call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2902\u001b[0m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[1;32m   2903\u001b[0m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[0;32m-> 2904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function(fn, args, kwargs)\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2906\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkw_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1193\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(fn\u001b[38;5;241m.\u001b[39mcall_function(\u001b[38;5;28mself\u001b[39m, args, kwargs))\n",
      "    \u001b[0;31m[... skipping similar frames: BaseUserFunctionVariable.call_function at line 293 (1 times), InliningInstructionTranslator.inline_call at line 3698 (1 times), InliningInstructionTranslator.inline_call_ at line 3901 (1 times), InstructionTranslatorBase.inline_user_function_return at line 1210 (1 times), InstructionTranslatorBase.run at line 1363 (1 times), InstructionTranslatorBase.step at line 1267 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:834\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_fn(\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_generic_context_managers:\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2910\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 2910\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inst)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2904\u001b[0m, in \u001b[0;36mInstructionTranslatorBase._call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2902\u001b[0m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[1;32m   2903\u001b[0m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[0;32m-> 2904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function(fn, args, kwargs)\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2906\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkw_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1193\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(fn\u001b[38;5;241m.\u001b[39mcall_function(\u001b[38;5;28mself\u001b[39m, args, kwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:529\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mside_effects\u001b[38;5;241m.\u001b[39mallow_side_effects_under_checkpoint(tx):\n\u001b[1;32m    528\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n\u001b[0;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcall_function(tx, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:293\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    289\u001b[0m     tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    290\u001b[0m     args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    291\u001b[0m     kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    292\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tx\u001b[38;5;241m.\u001b[39minline_user_function_return(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_args(), \u001b[38;5;241m*\u001b[39margs], kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1210\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minline_generator_function(fn, args, kwargs)\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InliningInstructionTranslator\u001b[38;5;241m.\u001b[39minline_call(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3698\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   3696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[1;32m   3697\u001b[0m     tracer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_inline_tracer(parent, func, args, kwargs)\n\u001b[0;32m-> 3698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minline_call_()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:3901\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3899\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3900\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 3901\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3903\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1363\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer\n\u001b[0;32m-> 1363\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep():\n\u001b[1;32m   1364\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1267\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_table[inst\u001b[38;5;241m.\u001b[39mopcode](\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2258\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.LOAD_METHOD\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mLOAD_METHOD\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 2258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr(inst)\n\u001b[1;32m   2259\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m13\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2282\u001b[0m, in \u001b[0;36mInstructionTranslatorBase._load_attr\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load_attr\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[1;32m   2281\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 2282\u001b[0m     result \u001b[38;5;241m=\u001b[39m BuiltinVariable(\u001b[38;5;28mgetattr\u001b[39m)\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m   2283\u001b[0m         \u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   2284\u001b[0m         [obj, ConstantVariable\u001b[38;5;241m.\u001b[39mcreate(inst\u001b[38;5;241m.\u001b[39margval)],\n\u001b[1;32m   2285\u001b[0m         {},\n\u001b[1;32m   2286\u001b[0m     )\n\u001b[1;32m   2287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py:1189\u001b[0m, in \u001b[0;36mBuiltinVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handler:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function_handler_cache[key] \u001b[38;5;241m=\u001b[39m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_handler(\n\u001b[1;32m   1187\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, [\u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args], \u001b[38;5;28mbool\u001b[39m(kwargs)\n\u001b[1;32m   1188\u001b[0m     )\n\u001b[0;32m-> 1189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handler(tx, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py:821\u001b[0m, in \u001b[0;36mBuiltinVariable._make_handler.<locals>.<lambda>\u001b[0;34m(tx, args, kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m handlers: \u001b[38;5;28mlist\u001b[39m[_HandlerCallback] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28missubclass\u001b[39m(t, LazyVariableTracker) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m arg_types):\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m tx, args, kwargs: obj\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    822\u001b[0m         tx, [v\u001b[38;5;241m.\u001b[39mrealize() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m args], kwargs\n\u001b[1;32m    823\u001b[0m     )\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(fn) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28missubclass\u001b[39m(fn, \u001b[38;5;167;01mException\u001b[39;00m)\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;66;03m# GeneratorExit doesn't inherit from Exception\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m\n\u001b[1;32m    831\u001b[0m ):\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_exception_class_object\u001b[39m(\n\u001b[1;32m    834\u001b[0m         tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, kwargs\n\u001b[1;32m    835\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py:1189\u001b[0m, in \u001b[0;36mBuiltinVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handler:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function_handler_cache[key] \u001b[38;5;241m=\u001b[39m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_handler(\n\u001b[1;32m   1187\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, [\u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args], \u001b[38;5;28mbool\u001b[39m(kwargs)\n\u001b[1;32m   1188\u001b[0m     )\n\u001b[0;32m-> 1189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handler(tx, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py:1009\u001b[0m, in \u001b[0;36mBuiltinVariable._make_handler.<locals>.builtin_dispatch\u001b[0;34m(tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuiltin_dispatch\u001b[39m(tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, kwargs):\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m-> 1009\u001b[0m         rv \u001b[38;5;241m=\u001b[39m fn(tx, args, kwargs)\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m rv:\n\u001b[1;32m   1011\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m rv\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py:887\u001b[0m, in \u001b[0;36mBuiltinVariable._make_handler.<locals>.call_self_handler\u001b[0;34m(tx, args, kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_self_handler\u001b[39m(tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, kwargs):\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 887\u001b[0m         result \u001b[38;5;241m=\u001b[39m self_handler(tx, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    889\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py:2099\u001b[0m, in \u001b[0;36mBuiltinVariable.call_getattr\u001b[0;34m(self, tx, obj, name_var, default)\u001b[0m\n\u001b[1;32m   2091\u001b[0m         unimplemented_v2(\n\u001b[1;32m   2092\u001b[0m             gb_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to wrap sparse Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2093\u001b[0m             context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2094\u001b[0m             explanation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.compile does not support sparse Tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2095\u001b[0m             hints\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m*\u001b[39mgraph_break_hints\u001b[38;5;241m.\u001b[39mSUPPORTABLE],\n\u001b[1;32m   2096\u001b[0m         )\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mvar_getattr(tx, name)\n\u001b[1;32m   2100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mGetAttrVariable(obj, name, source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:436\u001b[0m, in \u001b[0;36mTensorVariable.var_getattr\u001b[0;34m(self, tx, name)\u001b[0m\n\u001b[1;32m    430\u001b[0m         install_guard(\n\u001b[1;32m    431\u001b[0m             AttrSource(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource, name)\u001b[38;5;241m.\u001b[39mmake_guard(GuardBuilder\u001b[38;5;241m.\u001b[39mHASATTR)\n\u001b[1;32m    432\u001b[0m         )\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ConstantVariable(ret_val)\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvar_getattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m, name):\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_strict_mode(tx):\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strict_mode_banned_ops():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import StochasticDepth\n",
    "\n",
    "# --- CBAM: The Advanced Attention Module (Channel + Spatial) ---\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        # Use 1x1 convolutions as a shared MLP\n",
    "        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "                               nn.SiLU(),\n",
    "                               nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(in_planes, ratio)\n",
    "        self.sa = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply channel attention, then spatial attention\n",
    "        x = self.ca(x) * x\n",
    "        x = self.sa(x) * x\n",
    "        return x\n",
    "\n",
    "# --- The SOTA Residual Block ---\n",
    "class SOTA_ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, sd_prob=0.0):\n",
    "        super(SOTA_ResidualBlock, self).__init__()\n",
    "        self.silu = nn.SiLU(inplace=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.cbam = CBAM(out_channels)\n",
    "        self.stochastic_depth = StochasticDepth(sd_prob, \"row\")\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x) # Get the shortcut connection first\n",
    "\n",
    "        out = self.silu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.cbam(out)\n",
    "        \n",
    "        # Apply stochastic depth to the main path\n",
    "        out = self.stochastic_depth(out)\n",
    "        \n",
    "        # Add the identity and apply the final activation\n",
    "        out += identity\n",
    "        out = self.silu(out)\n",
    "        return out\n",
    "\n",
    "class SOTA_CustomCNN(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=116, sd_probs=None):\n",
    "        super(SOTA_CustomCNN, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        if sd_probs is None:\n",
    "            sd_probs = [0.0] * 4\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, self.in_channels // 2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.in_channels // 2),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(self.in_channels // 2, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.in_channels),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, sd_prob=sd_probs[0])\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, sd_prob=sd_probs[1])\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, sd_prob=sd_probs[2])\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, sd_prob=sd_probs[3])\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride, sd_prob):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, s, sd_prob=sd_prob))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stem(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "# --- Instantiate the SOTA model ---\n",
    "# Create a linear schedule for the stochastic depth probabilities.\n",
    "# Deeper layers are more likely to be dropped, which is a common and effective practice.\n",
    "# --- Instantiate the SOTA model ---\n",
    "sd_probs = [x.item() for x in torch.linspace(0, 0.1, 4)]\n",
    "# ‚ú® Use the new, clear class name ‚ú®\n",
    "model = SOTA_CustomCNN(SOTA_ResidualBlock, [2, 2, 2, 2], num_classes=NUM_CLASSES, sd_probs=sd_probs)\n",
    "\n",
    "print(\"\\n--- Attempting Model Compilation ---\")\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    if platform.system() == \"Windows\":\n",
    "        print(\"Windows OS detected. Skipping torch.compile() due to known issues.\")\n",
    "    else:\n",
    "        print(f\"{platform.system()} OS detected. Attempting to compile the model...\")\n",
    "        try:\n",
    "            model = torch.compile(model)\n",
    "            print(\"Model compiled successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Model compilation failed: {e}. Continuing without compilation.\")\n",
    "else:\n",
    "    print(f\"PyTorch version {torch.__version__} is less than 2.0. Skipping torch.compile.\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"--- Model Summary for {MODEL_NAME} (before compilation) ---\")\n",
    "summary_str = io.StringIO()\n",
    "with contextlib.redirect_stdout(summary_str):\n",
    "    summary(model, input_size=(3, IMAGE_SIZE_PHASE2, IMAGE_SIZE_PHASE2), device=\"cpu\") # Use CPU for summary\n",
    "model_summary = summary_str.getvalue()\n",
    "with open(os.path.join(RESULTS_DIR, \"model_summary.txt\"), \"w\") as f:\n",
    "    f.write(model_summary)\n",
    "print(\"Model summary saved to model_summary.txt\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "\n",
    "\n",
    "# --- Move the model to the GPU ---\n",
    "model = model.to(device)\n",
    "print(f\"Model moved to {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Creating a WeightedRandomSampler to handle data imbalance ---\")\n",
    "\n",
    "# Get the count of images in each class for the training set\n",
    "class_counts = np.array([0] * len(image_datasets_phase1['train'].classes))\n",
    "for _, label in image_datasets_phase1['train'].samples:\n",
    "    class_counts[label] += 1\n",
    "\n",
    "# Calculate weights for each sample (not each class)\n",
    "# weight = 1 / (number of samples in that sample's class)\n",
    "sample_weights = [0] * len(image_datasets_phase1['train'])\n",
    "for i, (image_path, label) in enumerate(image_datasets_phase1['train'].samples):\n",
    "    class_weight = 1. / class_counts[label]\n",
    "    sample_weights[i] = class_weight\n",
    "\n",
    "# Create the sampler\n",
    "sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    weights=torch.DoubleTensor(sample_weights),\n",
    "    num_samples=len(image_datasets_phase1['train']),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "print(\"WeightedRandomSampler created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_phase1 = {\n",
    "    'train': DataLoader(image_datasets_phase1['train'], batch_size=BATCH_SIZE_PHASE1, \n",
    "                        sampler=sampler, num_workers=NUM_WORKERS, pin_memory=True), # Use sampler, shuffle=False\n",
    "    'val': DataLoader(image_datasets_phase1['val'], batch_size=BATCH_SIZE_PHASE1, \n",
    "                      shuffle=False, num_workers=NUM_WORKERS, pin_memory=True) # Val should not be shuffled\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr_finder_save(lr_finder, save_path):\n",
    "    lr_finder.plot()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"LR Finder plot saved to {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.148317,
     "end_time": "2020-12-15T06:54:33.881587",
     "exception": false,
     "start_time": "2020-12-15T06:54:33.733270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üèóÔ∏è Modelling üèóÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.149596,
     "end_time": "2020-12-15T06:54:34.178417",
     "exception": false,
     "start_time": "2020-12-15T06:54:34.028821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It is advisable to use GPU instead of CPU when dealing with images dataset because CPUs are generalized for general purpose and GPUs are optimized for training deep learning models as they can process multiple computations simultaneously. They have a large number of cores, which allows for better computation of multiple parallel processes. Additionally, computations in deep learning need to handle huge amounts of data ‚Äî this makes a GPU‚Äôs memory bandwidth most suitable.\n",
    "To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_lr_finder import LRFinder\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\n--- Finding Optimal LR for a batch size of {BATCH_SIZE_PHASE1} ---\")\n",
    "\n",
    "finder_model = SOTA_CustomCNN(SOTA_ResidualBlock, [2, 2, 2, 2], num_classes=NUM_CLASSES).to(device)\n",
    "temp_optimizer = optim.AdamW(finder_model.parameters(), lr=1e-7, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "lr_finder = LRFinder(finder_model, temp_optimizer, criterion, device=device)\n",
    "\n",
    "# ‚ú® Pass the accumulation_steps to the finder ‚ú®\n",
    "lr_finder.range_test(\n",
    "    dataloaders_phase1['train'], \n",
    "    end_lr=1, \n",
    "    num_iter=200, \n",
    "    accumulation_steps=accumulation_steps # <-- The key addition\n",
    ")\n",
    "\n",
    "# The rest of the logic remains the same\n",
    "losses = np.array(lr_finder.history[\"loss\"])\n",
    "lrs = np.array(lr_finder.history[\"lr\"])\n",
    "min_loss_idx = np.argmin(losses)\n",
    "suggested_lr_phase1 = lrs[min_loss_idx] / 10\n",
    "\n",
    "lr_finder.plot()\n",
    "lr_finder.reset()\n",
    "\n",
    "print(f\"\\nLR Finder suggests a learning rate of: {suggested_lr_phase1:.2e} for the virtual batch size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE_PHASE1 = suggested_lr_phase1\n",
    "\n",
    "\n",
    "# OneCycleLR is excellent for training from scratch\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE_PHASE1, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                          max_lr=LEARNING_RATE_PHASE1, \n",
    "                                          epochs=NUM_EPOCHS_PHASE1, \n",
    "                                          steps_per_epoch=len(dataloaders_phase1['train']))\n",
    "\n",
    "print(f\"Phase 1 configured with AdamW, OneCycleLR, and an optimal max_lr of {LEARNING_RATE_PHASE1:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_CHECKPOINT = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# THE DEFINITIVE, PRODUCTION-READY train_model FUNCTION\n",
    "# ===================================================================\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, \n",
    "                dataloaders, dataset_sizes, checkpoint_path, log_path,\n",
    "                num_epochs=25, start_epoch=0, \n",
    "                early_stopping_patience=5, load_optimizer_state=False):\n",
    "    \n",
    "    since = time.time()\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lrs': []}\n",
    "\n",
    "    writer = SummaryWriter(f'runs/{MODEL_NAME}')\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(device.type == 'cuda'))\n",
    "\n",
    "    # --- Robust Checkpoint Loading ---\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint found at {checkpoint_path}. Loading...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        # ‚ú® THE FINAL FIX: Check for the '_orig_mod' attribute directly ‚ú®\n",
    "        # This is the most reliable way to know if the model in memory is compiled.\n",
    "        model_to_load = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "        \n",
    "        # The saved state_dict is always from an uncompiled model, so no cleaning is needed here.\n",
    "        model_to_load.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        best_acc = checkpoint.get('accuracy', 0.0)\n",
    "        print(f\"Loaded best accuracy from previous run: {best_acc:.4f}\")\n",
    "        \n",
    "        if load_optimizer_state:\n",
    "            print(\"Loading optimizer and scheduler state to resume training...\")\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "            print(f\"Resuming training from Epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(\"Starting a new training stage.\")\n",
    "\n",
    "    # --- Main Training Loop ---\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}'); print('-' * 10)\n",
    "\n",
    "        # ======================== 1. TRAINING PHASE ========================\n",
    "        model.train()\n",
    "        running_loss, running_corrects = 0.0, 0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        for inputs, labels in tqdm(dataloaders['train'], desc=\"Train Phase\"):\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                with torch.amp.autocast('cuda', enabled=(device.type == 'cuda')):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_train_loss = running_loss / dataset_sizes['train']\n",
    "        epoch_train_acc = (running_corrects.double() / dataset_sizes['train']).item()\n",
    "        \n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        writer.add_scalar('Loss/train', epoch_train_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', epoch_train_acc, epoch)\n",
    "        wandb.log({\"Epoch\": epoch, \"Train Loss\": epoch_train_loss, \"Train Acc\": epoch_train_acc})\n",
    "        print(f'Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n",
    "\n",
    "        # ======================== 2. VALIDATION PHASE ========================\n",
    "        model.eval()\n",
    "        running_loss, running_corrects = 0.0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(dataloaders['val'], desc=\"Validation Phase\"):\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                with torch.amp.autocast('cuda', enabled=(device.type == 'cuda')):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_val_loss = running_loss / dataset_sizes['val']\n",
    "        epoch_val_acc = (running_corrects.double() / dataset_sizes['val']).item()\n",
    "\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        writer.add_scalar('Loss/val', epoch_val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/val', epoch_val_acc, epoch)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['lrs'].append(current_lr)\n",
    "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
    "        wandb.log({\"Epoch\": epoch, \"Val Loss\": epoch_val_loss, \"Val Acc\": epoch_val_acc, \"Learning Rate\": current_lr})\n",
    "        print(f'Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')\n",
    "\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(epoch_val_loss)\n",
    "\n",
    "        # ======================== 3. END-OF-EPOCH TASKS ========================\n",
    "        \n",
    "        # --- CSV Logging ---\n",
    "        # ‚ú® FIX: Ensure log directory exists before writing ‚ú®\n",
    "        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "        new_log_data = {\n",
    "            'epoch': [epoch + 1], 'train_loss': [epoch_train_loss], 'train_acc': [epoch_train_acc],\n",
    "            'val_loss': [epoch_val_loss], 'val_acc': [epoch_val_acc], 'lr': [current_lr]\n",
    "        }\n",
    "        new_log_df = pd.DataFrame(new_log_data)\n",
    "        if not os.path.exists(log_path):\n",
    "            new_log_df.to_csv(log_path, index=False)\n",
    "        else:\n",
    "            new_log_df.to_csv(log_path, mode='a', header=False, index=False)\n",
    "        print(f\"Epoch {epoch+1} results logged to {log_path}\")\n",
    "\n",
    "        # --- Early Stopping & Checkpointing ---\n",
    "        if epoch_val_acc > best_acc:\n",
    "            best_acc = epoch_val_acc\n",
    "            epochs_no_improve = 0\n",
    "            model_to_save = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "            best_model_wts = model_to_save.state_dict()\n",
    "            \n",
    "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "            torch.save({'epoch': epoch, 'model_state_dict': best_model_wts, 'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(), 'loss': epoch_val_loss, 'accuracy': best_acc}, checkpoint_path)\n",
    "            print(f\"New best model saved to {checkpoint_path} with accuracy: {best_acc:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered after {epochs_no_improve} epochs with no improvement.\")\n",
    "                # Load the best weights before returning\n",
    "                model_to_load = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "                model_to_load.load_state_dict(best_model_wts)\n",
    "                writer.close()\n",
    "                return model, history\n",
    "        \n",
    "        print()\n",
    "\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}') # Use .4f for better formatting\n",
    "\n",
    "    # ‚ú® THE FIX: Correctly load the best weights before returning ‚ú®\n",
    "    # The 'best_model_wts' variable already holds the state_dict of the best performing model.\n",
    "    # We just need to load it into the correct underlying model.\n",
    "    \n",
    "    print(\"Loading best model weights before returning...\")\n",
    "    model_to_load = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "    model_to_load.load_state_dict(best_model_wts)\n",
    "    \n",
    "    writer.close()\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Maximum Performance Configuration ---\")\n",
    "print(f\"Phase 1: Training on {IMAGE_SIZE_PHASE1}x{IMAGE_SIZE_PHASE1} images with batch size {BATCH_SIZE_PHASE1}\")\n",
    "print(f\"Phase 2: Fine-tuning on {IMAGE_SIZE_PHASE2}x{IMAGE_SIZE_PHASE2} images with batch size {BATCH_SIZE_PHASE2}\")\n",
    "print(f\"Using {NUM_WORKERS} workers for data loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_phase1 = os.path.join(CHECKPOINT_DIR, 'best_model_phase1.pth')\n",
    "plot_lr_finder_save(lr_finder, os.path.join(RESULTS_DIR, \"lr_finder_phase1.png\"))\n",
    "log_path_phase1 = os.path.join(RESULTS_DIR, 'training_log_phase1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=f\"{MODEL_NAME}_Phase1_{int(time.time())}\", # A unique name for this run\n",
    "    config={\n",
    "        \"architecture\": MODEL_NAME,\n",
    "        \"phase\": 1,\n",
    "        \"image_size\": IMAGE_SIZE_PHASE1,\n",
    "        \"batch_size\": BATCH_SIZE_PHASE1,\n",
    "        \"epochs\": NUM_EPOCHS_PHASE1,\n",
    "        \"sampler\": \"WeightedRandomSampler\" # Good to log this\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history1 = train_model(model, criterion, optimizer, scheduler,\n",
    "                              dataloaders=dataloaders_phase1,\n",
    "                              dataset_sizes=dataset_sizes_phase1,\n",
    "                              checkpoint_path=checkpoint_path_phase1,\n",
    "                              log_path=log_path_phase1,\n",
    "                              num_epochs=NUM_EPOCHS_PHASE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- PHASE 2: Starting Setup for Fine-tuning on {IMAGE_SIZE_PHASE2} x {IMAGE_SIZE_PHASE2} Images ---\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Using batch size of {BATCH_SIZE_PHASE2} for full-resolution training.\")\n",
    "\n",
    "data_transforms_phase2 = {\n",
    "    'train': transforms.Compose([\n",
    "        # For fine-tuning, we use the full resolution\n",
    "        transforms.RandomResizedCrop(IMAGE_SIZE_PHASE2, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.TrivialAugmentWide(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE_PHASE2 + 32),\n",
    "        transforms.CenterCrop(IMAGE_SIZE_PHASE2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Create the datasets and dataloaders for this phase\n",
    "image_datasets_phase2 = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x), data_transforms_phase2[x]) for x in ['train', 'val']}\n",
    "dataloaders_phase2 = {x: DataLoader(image_datasets_phase2[x], batch_size=BATCH_SIZE_PHASE2, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True) for x in ['train', 'val']}\n",
    "dataset_sizes_phase2 = {x: len(image_datasets_phase2[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Finding Optimal LR for Fine-Tuning ---\")\n",
    "\n",
    "finder_model_phase2 = SOTA_CustomCNN(SOTA_ResidualBlock, [2, 2, 2, 2], num_classes=NUM_CLASSES, sd_probs=sd_probs).to(device)\n",
    "\n",
    "# 2. Load the checkpoint from the compiled Phase 1 model\n",
    "checkpoint_path_phase1 = os.path.join(CHECKPOINT_DIR, 'best_model_phase1.pth')\n",
    "# We still need weights_only=False because of the NumPy scalar issue\n",
    "checkpoint = torch.load(checkpoint_path_phase1, weights_only=False)\n",
    "compiled_state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# ‚ú® THE FIX: Clean the keys in the state_dict ‚ú®\n",
    "# Create a new dictionary and remove the '_orig_mod.' prefix from each key.\n",
    "clean_state_dict = {}\n",
    "for k, v in compiled_state_dict.items():\n",
    "    # The key in the compiled model is '_orig_mod.layer1...', we want 'layer1...'\n",
    "    if k.startswith('_orig_mod.'):\n",
    "        new_key = k[len('_orig_mod.'):]\n",
    "        clean_state_dict[new_key] = v\n",
    "    else:\n",
    "        # If for some reason a key doesn't have the prefix, keep it as is\n",
    "        clean_state_dict[k] = v\n",
    "\n",
    "# 3. Load the CLEANED state_dict into the fresh, uncompiled model\n",
    "finder_model_phase2.load_state_dict(clean_state_dict)\n",
    "print(f\"LR Finder model loaded with cleaned weights from Phase 1.\")\n",
    "# Set up the finder\n",
    "temp_optimizer = optim.AdamW(finder_model_phase2.parameters(), lr=1e-7, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "lr_finder = LRFinder(finder_model_phase2, temp_optimizer, criterion, device=device)\n",
    "\n",
    "# Run the test on the new, high-resolution data\n",
    "lr_finder.range_test(dataloaders_phase2['train'], end_lr=1, num_iter=200)\n",
    "\n",
    "# Capture the suggestion automatically using the compatible method\n",
    "losses = np.array(lr_finder.history[\"loss\"])\n",
    "lrs = np.array(lr_finder.history[\"lr\"])\n",
    "min_loss_idx = np.argmin(losses)\n",
    "suggested_lr_phase2 = lrs[min_loss_idx] / 10\n",
    "\n",
    "# Plot the results and clean up\n",
    "lr_finder.plot()\n",
    "lr_finder.reset()\n",
    "\n",
    "print(f\"\\nLR Finder suggests a fine-tuning learning rate of: {suggested_lr_phase2:.2e}\")\n",
    "print(\"This value will now be used to configure the final training phase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE_PHASE2 = suggested_lr_phase2\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE_PHASE2, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3)\n",
    "\n",
    "print(f\"Phase 2 configured with AdamW (LR={LEARNING_RATE_PHASE2:.2e}) and ReduceLROnPlateau scheduler.\")\n",
    "\n",
    "\n",
    "plot_lr_finder_save(lr_finder, os.path.join(RESULTS_DIR, \"lr_finder_phase2.png\"))\n",
    "checkpoint_path_phase2 = os.path.join(CHECKPOINT_DIR, 'best_model_final.pth')\n",
    "log_path_phase2 = os.path.join(RESULTS_DIR, 'training_log_phase2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=f\"{MODEL_NAME}_Phase2_{int(time.time())}\",\n",
    "    config={\n",
    "        \"architecture\": MODEL_NAME,\n",
    "        \"phase\": 2,\n",
    "        \"image_size\": IMAGE_SIZE_PHASE2,\n",
    "        \"batch_size\": BATCH_SIZE_PHASE2,\n",
    "        \"epochs\": NUM_EPOCHS_PHASE2,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history2 = train_model(model, criterion, optimizer, scheduler,\n",
    "                              dataloaders=dataloaders_phase2,\n",
    "                              dataset_sizes=dataset_sizes_phase2,\n",
    "                              checkpoint_path=checkpoint_path_phase2,\n",
    "                              log_path=log_path_phase2,\n",
    "                              num_epochs=NUM_EPOCHS_PHASE2,\n",
    "                              load_optimizer_state=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_history(history1, history2, model_name, save_path):\n",
    "    \"\"\"\n",
    "    Combines and plots the training histories from two separate phases.\n",
    "    \"\"\"\n",
    "    # Combine the metrics from both history objects\n",
    "    full_history = {\n",
    "        'train_acc': history1['train_acc'] + history2['train_acc'],\n",
    "        'val_acc': history1['val_acc'] + history2['val_acc'],\n",
    "        'train_loss': history1['train_loss'] + history2['train_loss'],\n",
    "        'val_loss': history1['val_loss'] + history2['val_loss'],\n",
    "        'lrs': history1.get('lrs', []) + history2.get('lrs', [])\n",
    "    }\n",
    "    \n",
    "    num_epochs_phase1 = len(history1['train_acc'])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 8))\n",
    "    fig.suptitle(f'Combined Training History for {model_name}', fontsize=18)\n",
    "\n",
    "    # --- Accuracy Plot ---\n",
    "    ax1.plot(full_history['train_acc'], label='Train Acc', marker='.', linestyle='-')\n",
    "    ax1.plot(full_history['val_acc'], label='Val Acc', marker='.', linestyle='-')\n",
    "    ax1.set_title('Model Accuracy', fontsize=14)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Add a vertical line to show the transition between phases\n",
    "    ax1.axvline(x=num_epochs_phase1 - 1, color='grey', linestyle='--', linewidth=2, label='Phase 1/2 Transition')\n",
    "    \n",
    "    # Annotate the best validation accuracy\n",
    "    best_val_acc = max(full_history['val_acc'])\n",
    "    best_epoch = np.argmax(full_history['val_acc'])\n",
    "    ax1.annotate(f'Best Val Acc: {best_val_acc:.4f}\\nEpoch {best_epoch+1}',\n",
    "                 xy=(best_epoch, best_val_acc),\n",
    "                 xytext=(best_epoch, best_val_acc - 0.1),\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "                 ha='center', fontsize=10)\n",
    "    ax1.legend()\n",
    "\n",
    "    # --- Loss Plot ---\n",
    "    ax2.plot(full_history['train_loss'], label='Train Loss', marker='.', linestyle='-')\n",
    "    ax2.plot(full_history['val_loss'], label='Val Loss', marker='.', linestyle='-')\n",
    "    ax2.set_title('Model Loss', fontsize=14)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax2.axvline(x=num_epochs_phase1 - 1, color='grey', linestyle='--', linewidth=2, label='Phase 1/2 Transition')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "    # --- Learning Rate Plot ---\n",
    "    if full_history['lrs']:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.plot(full_history['lrs'])\n",
    "        plt.title('Learning Rate Schedule (per batch)', fontsize=14)\n",
    "        plt.xlabel('Batch # (cumulative)')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        # Add a vertical line for the phase transition\n",
    "        num_batches_phase1 = len(history1.get('lrs', []))\n",
    "        plt.axvline(x=num_batches_phase1, color='grey', linestyle='--', linewidth=2, label='Phase 1/2 Transition')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_learning_rate_schedule(history):\n",
    "    lrs = history.get('lrs', [])\n",
    "    if len(lrs) > 0:\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.plot(lrs)\n",
    "        plt.title('Learning Rate (per batch)')\n",
    "        plt.xlabel('Batch # (cumulative)')\n",
    "        plt.ylabel('LR')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No LR data to plot.')\n",
    "\n",
    "# --- Evaluation helpers ---\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc='Eval'):\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_labels, all_preds\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(labels, preds, class_names, normalize=True, max_classes_display=50):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    # If too many classes, show a subset heatmap legend only\n",
    "    if len(class_names) > max_classes_display:\n",
    "        print(f\"Too many classes ({len(class_names)}). Showing aggregated stats.\")\n",
    "        avg_diag = np.nanmean(np.diag(cm))\n",
    "        print(f\"Mean per-class accuracy: {avg_diag:.4f}\")\n",
    "        return\n",
    "    plt.figure(figsize=(min(1+0.3*len(class_names), 25), min(1+0.3*len(class_names), 25)))\n",
    "    sns.heatmap(cm, cmap='viridis', xticklabels=class_names, yticklabels=class_names, fmt='.2f' if normalize else 'd')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix' + (' (Normalized)' if normalize else ''))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_misclassified(model, dataloader, class_names, device, n=12):\n",
    "    model.eval()\n",
    "    images = []\n",
    "    labels_true = []\n",
    "    labels_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            mismatch = preds != labels.to(device)\n",
    "            if mismatch.any():\n",
    "                for i in torch.where(mismatch)[0]:\n",
    "                    images.append(inputs[i].cpu())\n",
    "                    labels_true.append(labels[i].item())\n",
    "                    labels_pred.append(preds[i].item())\n",
    "                    if len(images) >= n:\n",
    "                        break\n",
    "            if len(images) >= n:\n",
    "                break\n",
    "    if len(images) == 0:\n",
    "        print('No misclassifications found in this subset.')\n",
    "        return\n",
    "    # Denormalize for display\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    cols = 4\n",
    "    rows = int(np.ceil(len(images)/cols))\n",
    "    plt.figure(figsize=(cols*4, rows*4))\n",
    "    for idx, img in enumerate(images):\n",
    "        img_dn = img*std + mean\n",
    "        img_np = np.clip(img_dn.permute(1,2,0).numpy(), 0, 1)\n",
    "        ax = plt.subplot(rows, cols, idx+1)\n",
    "        ax.imshow(img_np)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"T: {class_names[labels_true[idx]][:15]}\\nP: {class_names[labels_pred[idx]][:15]}\", fontsize=9)\n",
    "    plt.suptitle('Sample Misclassified Images', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Grad-CAM (lightweight) ---\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        target_layer.register_forward_hook(self.forward_hook)\n",
    "        target_layer.register_backward_hook(self.backward_hook)\n",
    "\n",
    "    def forward_hook(self, module, inp, out):\n",
    "        self.activations = out.detach()\n",
    "\n",
    "    def backward_hook(self, module, grad_in, grad_out):\n",
    "        self.gradients = grad_out[0].detach()\n",
    "\n",
    "    def generate(self, input_tensor, class_idx=None):\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_tensor)\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1)\n",
    "        selected = output[0, class_idx]\n",
    "        selected.backward(retain_graph=True)\n",
    "        weights = self.gradients.mean(dim=[2,3], keepdim=True)\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        cam = torch.nn.functional.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
    "        cam_min, cam_max = cam.min(), cam.max()\n",
    "        cam = (cam - cam_min)/(cam_max - cam_min + 1e-8)\n",
    "        return cam[0,0].cpu().numpy()\n",
    "\n",
    "\n",
    "def plot_combined_history_save(history1, history2, model_name, save_path):\n",
    "    \"\"\"\n",
    "    Combines and plots the training histories from two separate phases.\n",
    "    \"\"\"\n",
    "    # Combine the metrics from both history objects\n",
    "    full_history = {\n",
    "        'train_acc': history1['train_acc'] + history2['train_acc'],\n",
    "        'val_acc': history1['val_acc'] + history2['val_acc'],\n",
    "        'train_loss': history1['train_loss'] + history2['train_loss'],\n",
    "        'val_loss': history1['val_loss'] + history2['val_loss'],\n",
    "        'lrs': history1.get('lrs', []) + history2.get('lrs', [])\n",
    "    }\n",
    "    \n",
    "    num_epochs_phase1 = len(history1['train_acc'])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 8))\n",
    "    fig.suptitle(f'Combined Training History for {model_name}', fontsize=18)\n",
    "\n",
    "    # --- Accuracy Plot ---\n",
    "    ax1.plot(full_history['train_acc'], label='Train Acc', marker='.', linestyle='-')\n",
    "    ax1.plot(full_history['val_acc'], label='Val Acc', marker='.', linestyle='-')\n",
    "    ax1.set_title('Model Accuracy', fontsize=14)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Add a vertical line to show the transition between phases\n",
    "    ax1.axvline(x=num_epochs_phase1 - 1, color='grey', linestyle='--', linewidth=2, label='Phase 1/2 Transition')\n",
    "    \n",
    "    # Annotate the best validation accuracy\n",
    "    best_val_acc = max(full_history['val_acc'])\n",
    "    best_epoch = np.argmax(full_history['val_acc'])\n",
    "    ax1.annotate(f'Best Val Acc: {best_val_acc:.4f}\\nEpoch {best_epoch+1}',\n",
    "                 xy=(best_epoch, best_val_acc),\n",
    "                 xytext=(best_epoch, best_val_acc - 0.1),\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "                 ha='center', fontsize=10)\n",
    "    ax1.legend()\n",
    "\n",
    "    # --- Loss Plot ---\n",
    "    ax2.plot(full_history['train_loss'], label='Train Loss', marker='.', linestyle='-')\n",
    "    ax2.plot(full_history['val_loss'], label='Val Loss', marker='.', linestyle='-')\n",
    "    ax2.set_title('Model Loss', fontsize=14)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax2.axvline(x=num_epochs_phase1 - 1, color='grey', linestyle='--', linewidth=2, label='Phase 1/2 Transition')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "    # --- Learning Rate Plot ---\n",
    "    if full_history['lrs']:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.plot(full_history['lrs'])\n",
    "        plt.title('Learning Rate Schedule (per batch)', fontsize=14)\n",
    "        plt.xlabel('Batch # (cumulative)')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        # Add a vertical line for the phase transition\n",
    "        num_batches_phase1 = len(history1.get('lrs', []))\n",
    "        plt.axvline(x=num_batches_phase1, color='grey', linestyle='--', linewidth=2, label='Phase 1/2 Transition')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    print(f\"Combined history plot saved to {save_path}\")\n",
    "    plt.close() \n",
    "\n",
    "\n",
    "    \n",
    "def visualize_gradcam(model, dataloader, class_names, device, n=3):\n",
    "    # Pick the last residual layer for Grad-CAM\n",
    "    target_layer = model.layer4[-1].bn2\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "\n",
    "    shown = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        for i in range(inputs.size(0)):\n",
    "            if shown >= n:\n",
    "                return\n",
    "            inp = inputs[i:i+1].to(device)\n",
    "            cam = gradcam.generate(inp)\n",
    "            img = (inputs[i]*std + mean).permute(1,2,0).numpy()\n",
    "            img = np.clip(img, 0, 1)\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(cam, cmap='jet', alpha=0.4)\n",
    "            pred_class = class_names[model(inp).argmax(1).item()]\n",
    "            plt.title(f\"True: {class_names[labels[i]]}\\nPred: {pred_class}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            shown += 1\n",
    "\n",
    "print(\"Visualization & evaluation helpers ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# FINAL PLOTTING, EVALUATION, AND LOGGING\n",
    "# ===================================================================\n",
    "\n",
    "# --- 1. Generate and Save Local Training Plots ---\n",
    "print(\"\\n--- Generating and Saving Local Training Plots ---\")\n",
    "# This saves the combined history plot to your results directory\n",
    "plot_combined_history_save(history1, history2, MODEL_NAME, os.path.join(RESULTS_DIR, \"training_history.png\"))\n",
    "\n",
    "\n",
    "# --- 2. Final Evaluation on the Unseen Test Set ---\n",
    "print(\"\\n--- FINAL EVALUATION: Running on the unseen test set ---\")\n",
    "\n",
    "# A. Load class names and create the test dataloader\n",
    "import json\n",
    "with open(os.path.join(RESULTS_DIR, 'class_names.json'), 'r') as f:\n",
    "    class_names = json.load(f)\n",
    "print(f\"Successfully loaded {len(class_names)} class names for evaluation.\")\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE_PHASE2 + 32),\n",
    "    transforms.CenterCrop(IMAGE_SIZE_PHASE2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "test_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, 'test'), test_transforms)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE_PHASE2, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "print(f\"Test dataloader created with {len(test_dataset)} images.\")\n",
    "\n",
    "# B. Instantiate and compile the final model architecture\n",
    "final_model = SOTA_CustomCNN(SOTA_ResidualBlock, [2, 2, 2, 2], num_classes=NUM_CLASSES, sd_probs=sd_probs)\n",
    "if int(torch.__version__.split('.')[0]) >= 2 and platform.system() != \"Windows\":\n",
    "    try:\n",
    "        final_model = torch.compile(final_model)\n",
    "        print(\"Final evaluation model compiled successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compile the final model: {e}\")\n",
    "final_model.to(device)\n",
    "\n",
    "# C. Load the single best model saved from the ENTIRE process\n",
    "final_checkpoint_path = os.path.join(CHECKPOINT_DIR, 'best_model_final.pth')\n",
    "print(f\"Loading best model for final evaluation from: {final_checkpoint_path}\")\n",
    "# Use the robust loading logic from the train_model function\n",
    "checkpoint = torch.load(final_checkpoint_path, map_location=device, weights_only=False)\n",
    "final_model_to_load = final_model._orig_mod if hasattr(final_model, '_orig_mod') else final_model\n",
    "final_model_to_load.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# D. Evaluate the model\n",
    "test_labels, test_preds = evaluate_model(final_model, test_dataloader, class_names, device)\n",
    "\n",
    "# E. Print and Save Local Reports\n",
    "print(\"\\n--- Final Test Set Classification Report ---\")\n",
    "report_str = classification_report(test_labels, test_preds, target_names=class_names, digits=4)\n",
    "print(report_str)\n",
    "with open(os.path.join(RESULTS_DIR, \"classification_report.txt\"), \"w\") as f:\n",
    "    f.write(report_str)\n",
    "print(\"Classification report saved to classification_report.txt\")\n",
    "\n",
    "print(\"\\n--- Final Test Set Confusion Matrix ---\")\n",
    "# This function needs to be modified to accept a save_path\n",
    "# plot_confusion_matrix_save(test_labels, test_preds, class_names, save_path=os.path.join(RESULTS_DIR, \"confusion_matrix.png\"))\n",
    "\n",
    "# --- 3. ‚ú® Log Rich Results to Weights & Biases ‚ú® ---\n",
    "print(\"\\n--- Logging final results to Weights & Biases ---\")\n",
    "\n",
    "# A. Log summary metrics from the classification report\n",
    "report_dict = classification_report(test_labels, test_preds, target_names=class_names, digits=4, output_dict=True)\n",
    "wandb.summary[\"Test Accuracy\"] = report_dict['accuracy']\n",
    "wandb.summary[\"Test Macro Avg F1-score\"] = report_dict['macro avg']['f1-score']\n",
    "wandb.summary[\"Test Weighted Avg F1-score\"] = report_dict['weighted avg']['f1-score']\n",
    "print(\"Logged summary test metrics to W&B.\")\n",
    "\n",
    "# B. Log the confusion matrix as an interactive plot\n",
    "# W&B has a built-in tool for this, which is better than a static image\n",
    "wandb.log({\"Confusion Matrix\": wandb.plot.confusion_matrix(\n",
    "    preds=test_preds,\n",
    "    y_true=test_labels,\n",
    "    class_names=class_names\n",
    ")})\n",
    "print(\"Logged interactive confusion matrix to W&B.\")\n",
    "\n",
    "# C. Log a table with misclassified examples\n",
    "# This is incredibly useful for debugging\n",
    "misclassified_table = wandb.Table(columns=[\"ID\", \"Image\", \"True Label\", \"Predicted Label\"])\n",
    "# Get a few misclassified examples\n",
    "model.eval()\n",
    "count = 0\n",
    "max_misclassified = 32 # Log up to 32 examples\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_dataloader):\n",
    "        if count >= max_misclassified: break\n",
    "        outputs = model(inputs.to(device))\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        mismatched_idx = torch.where(preds.cpu() != labels)[0]\n",
    "        for idx in mismatched_idx:\n",
    "            if count >= max_misclassified: break\n",
    "            misclassified_table.add_data(\n",
    "                f\"test_{i}_{idx}\",\n",
    "                wandb.Image(inputs[idx]),\n",
    "                class_names[labels[idx]],\n",
    "                class_names[preds[idx]]\n",
    "            )\n",
    "            count += 1\n",
    "wandb.log({\"Misclassified Examples\": misclassified_table})\n",
    "print(f\"Logged {count} misclassified examples to a W&B Table.\")\n",
    "\n",
    "# D. Save the final model file as a W&B Artifact for versioning and reproducibility\n",
    "print(\"Saving final model as a W&B Artifact...\")\n",
    "artifact = wandb.Artifact(f'{MODEL_NAME}-final', type='model', metadata=report_dict)\n",
    "artifact.add_file(final_checkpoint_path)\n",
    "run.log_artifact(artifact)\n",
    "print(\"Model artifact saved.\")\n",
    "\n",
    "# E. Finish the W&B run\n",
    "run.finish()\n",
    "print(\"W&B run finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "duration": 1424.697889,
   "end_time": "2020-12-15T07:16:24.493770",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-15T06:52:39.795881",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
