{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.059875,
     "end_time": "2020-12-15T06:52:44.191791",
     "exception": false,
     "start_time": "2020-12-15T06:52:44.131916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# â˜˜ï¸ PLANT DISEASE CLASSIFICATION USING Custom CNN â˜˜ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056728,
     "end_time": "2020-12-15T06:52:44.447613",
     "exception": false,
     "start_time": "2020-12-15T06:52:44.390885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Description of the dataset ðŸ“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057571,
     "end_time": "2020-12-15T06:52:44.675922",
     "exception": false,
     "start_time": "2020-12-15T06:52:44.618351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Our goal ðŸŽ¯\n",
    "Goal is clear and simple. We need to build a model, which can classify between healthy and diseased crop leaves and also if the crop have any disease, predict which disease is it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056957,
     "end_time": "2020-12-15T06:52:44.789985",
     "exception": false,
     "start_time": "2020-12-15T06:52:44.733028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Let's get started...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056658,
     "end_time": "2020-12-15T06:52:44.903338",
     "exception": false,
     "start_time": "2020-12-15T06:52:44.846680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057862,
     "end_time": "2020-12-15T06:52:45.017851",
     "exception": false,
     "start_time": "2020-12-15T06:52:44.959989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's import required modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058411,
     "end_time": "2020-12-15T06:52:54.403660",
     "exception": false,
     "start_time": "2020-12-15T06:52:54.345249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We would require torchsummary library to print the model's summary in keras style (nicely formatted and pretty to look) as Pytorch natively doesn't support that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:54.531023Z",
     "iopub.status.busy": "2020-12-15T06:52:54.530191Z",
     "iopub.status.idle": "2020-12-15T06:52:56.166219Z",
     "shell.execute_reply": "2020-12-15T06:52:56.164951Z"
    },
    "papermill": {
     "duration": 1.704433,
     "end_time": "2020-12-15T06:52:56.166377",
     "exception": false,
     "start_time": "2020-12-15T06:52:54.461944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.utils.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms, models\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.utils.data'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# This is a Jupyter magic command to display plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058296,
     "end_time": "2020-12-15T06:52:56.283998",
     "exception": false,
     "start_time": "2020-12-15T06:52:56.225702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ§­ Exploring the data ðŸ§­"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.05825,
     "end_time": "2020-12-15T06:52:56.400725",
     "exception": false,
     "start_time": "2020-12-15T06:52:56.342475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:56.522416Z",
     "iopub.status.busy": "2020-12-15T06:52:56.521802Z",
     "iopub.status.idle": "2020-12-15T06:52:56.536807Z",
     "shell.execute_reply": "2020-12-15T06:52:56.536213Z"
    },
    "papermill": {
     "duration": 0.07813,
     "end_time": "2020-12-15T06:52:56.536899",
     "exception": false,
     "start_time": "2020-12-15T06:52:56.458769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./new_data\"\n",
    "train_dir = DATA_DIR + \"/train\"\n",
    "valid_dir = DATA_DIR + \"/val\"\n",
    "test_dir = DATA_DIR + \"/test\"\n",
    "diseases = os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:56.660806Z",
     "iopub.status.busy": "2020-12-15T06:52:56.660131Z",
     "iopub.status.idle": "2020-12-15T06:52:56.663554Z",
     "shell.execute_reply": "2020-12-15T06:52:56.664320Z"
    },
    "papermill": {
     "duration": 0.068176,
     "end_time": "2020-12-15T06:52:56.664465",
     "exception": false,
     "start_time": "2020-12-15T06:52:56.596289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# printing the disease names\n",
    "print(diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:56.789812Z",
     "iopub.status.busy": "2020-12-15T06:52:56.789079Z",
     "iopub.status.idle": "2020-12-15T06:52:56.792917Z",
     "shell.execute_reply": "2020-12-15T06:52:56.792464Z"
    },
    "papermill": {
     "duration": 0.068791,
     "end_time": "2020-12-15T06:52:56.793019",
     "exception": false,
     "start_time": "2020-12-15T06:52:56.724228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Total disease classes are: {}\".format(len(diseases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"EfficientNetB3_FineTuned\" \n",
    "NUM_CLASSES = 116\n",
    "BATCH_SIZE = 32 # Increased to 128 for your RTX 4070. Lower to 64 if you get memory errors.\n",
    "NUM_EPOCHS = 25 # Increased epochs for a more thorough training run\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "CHECKPOINT_DIR = f\"./models/{MODEL_NAME}\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f'runs/{MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.069462,
     "end_time": "2020-12-15T06:52:57.055273",
     "exception": false,
     "start_time": "2020-12-15T06:52:56.985811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The above cell extract the number of unique plants and number of unique diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plants = []\n",
    "NumberOfDiseases = 0\n",
    "for plant in diseases:\n",
    "    if plant.split('__')[0] not in plants:\n",
    "        plants.append(plant.split('__')[0])\n",
    "    if plant.split('__')[1] != 'healthy':\n",
    "        NumberOfDiseases += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:57.183047Z",
     "iopub.status.busy": "2020-12-15T06:52:57.182397Z",
     "iopub.status.idle": "2020-12-15T06:52:57.186314Z",
     "shell.execute_reply": "2020-12-15T06:52:57.185696Z"
    },
    "papermill": {
     "duration": 0.068933,
     "end_time": "2020-12-15T06:52:57.186415",
     "exception": false,
     "start_time": "2020-12-15T06:52:57.117482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unique plants in the dataset\n",
    "print(f\"Unique Plants are: \\n{plants}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:57.313744Z",
     "iopub.status.busy": "2020-12-15T06:52:57.313076Z",
     "iopub.status.idle": "2020-12-15T06:52:57.316355Z",
     "shell.execute_reply": "2020-12-15T06:52:57.317088Z"
    },
    "papermill": {
     "duration": 0.069891,
     "end_time": "2020-12-15T06:52:57.317251",
     "exception": false,
     "start_time": "2020-12-15T06:52:57.247360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of unique plants\n",
    "print(\"Number of plants: {}\".format(len(plants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:57.444856Z",
     "iopub.status.busy": "2020-12-15T06:52:57.444130Z",
     "iopub.status.idle": "2020-12-15T06:52:57.447598Z",
     "shell.execute_reply": "2020-12-15T06:52:57.448386Z"
    },
    "papermill": {
     "duration": 0.069339,
     "end_time": "2020-12-15T06:52:57.448580",
     "exception": false,
     "start_time": "2020-12-15T06:52:57.379241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of unique diseases\n",
    "print(\"Number of diseases: {}\".format(NumberOfDiseases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = f\"./models/{MODEL_NAME}\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.062489,
     "end_time": "2020-12-15T06:52:57.574134",
     "exception": false,
     "start_time": "2020-12-15T06:52:57.511645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So we have images of leaves of 14 plants and while excluding healthy leaves, we have 26 types of images that show a particular disease in a particular plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:52:57.704194Z",
     "iopub.status.busy": "2020-12-15T06:52:57.703583Z",
     "iopub.status.idle": "2020-12-15T06:53:03.960106Z",
     "shell.execute_reply": "2020-12-15T06:53:03.960800Z"
    },
    "papermill": {
     "duration": 6.323955,
     "end_time": "2020-12-15T06:53:03.960987",
     "exception": false,
     "start_time": "2020-12-15T06:52:57.637032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of images for each disease\n",
    "nums = {}\n",
    "for disease in diseases:\n",
    "    nums[disease] = len(os.listdir(train_dir + '/' + disease))\n",
    "    \n",
    "# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\n",
    "\n",
    "img_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=[\"no. of images\"])\n",
    "img_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.090681,
     "end_time": "2020-12-15T06:53:04.148485",
     "exception": false,
     "start_time": "2020-12-15T06:53:04.057804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Visualizing the above information on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:53:04.334832Z",
     "iopub.status.busy": "2020-12-15T06:53:04.333992Z",
     "iopub.status.idle": "2020-12-15T06:53:04.860737Z",
     "shell.execute_reply": "2020-12-15T06:53:04.859673Z"
    },
    "papermill": {
     "duration": 0.623297,
     "end_time": "2020-12-15T06:53:04.860887",
     "exception": false,
     "start_time": "2020-12-15T06:53:04.237590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting number of images available for each disease\n",
    "index = [n for n in range(116)]\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(index, [n for n in nums.values()], width=0.3)\n",
    "plt.xlabel('Plants/Diseases', fontsize=10)\n",
    "plt.ylabel('No of images available', fontsize=10)\n",
    "plt.xticks(index, diseases, fontsize=5, rotation=90)\n",
    "plt.title('Images per each class of plant disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.068183,
     "end_time": "2020-12-15T06:53:04.997832",
     "exception": false,
     "start_time": "2020-12-15T06:53:04.929649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can see that the dataset is almost balanced for all classes, so we are good to go forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.073886,
     "end_time": "2020-12-15T06:53:05.152902",
     "exception": false,
     "start_time": "2020-12-15T06:53:05.079016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Images available for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:53:05.285106Z",
     "iopub.status.busy": "2020-12-15T06:53:05.284437Z",
     "iopub.status.idle": "2020-12-15T06:53:05.287876Z",
     "shell.execute_reply": "2020-12-15T06:53:05.288406Z"
    },
    "papermill": {
     "duration": 0.07225,
     "end_time": "2020-12-15T06:53:05.288530",
     "exception": false,
     "start_time": "2020-12-15T06:53:05.216280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_train = 0\n",
    "for value in nums.values():\n",
    "    n_train += value\n",
    "print(f\"There are {n_train} images for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.064118,
     "end_time": "2020-12-15T06:53:05.416780",
     "exception": false,
     "start_time": "2020-12-15T06:53:05.352662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ³ Data Preparation for training ðŸ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T06:53:05.548188Z",
     "iopub.status.busy": "2020-12-15T06:53:05.547365Z",
     "iopub.status.idle": "2020-12-15T06:54:23.286526Z",
     "shell.execute_reply": "2020-12-15T06:54:23.285629Z"
    },
    "papermill": {
     "duration": 77.805914,
     "end_time": "2020-12-15T06:54:23.286647",
     "exception": false,
     "start_time": "2020-12-15T06:53:05.480733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.TrivialAugmentWide(interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Loading data with STRONGER augmentations...\")\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=(x=='train'), num_workers=4, pin_memory=True) for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "print(f\"Dataset sizes: Train={dataset_sizes['train']}, Val={dataset_sizes['val']}, Test={dataset_sizes['test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('class_names.json', 'w') as f:\n",
    "    json.dump(class_names, f)\n",
    "print(\"Class names saved to class_names.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A \"Good and Complex\" Custom CNN Model Definition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "# --- Step 1: Load the pre-trained EfficientNet-B3 model ---\n",
    "# We use the recommended modern weights API.\n",
    "print(\"Loading pre-trained EfficientNet-B3 model...\")\n",
    "model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# --- Step 2: Freeze all the parameters in the pre-trained model ---\n",
    "# This is crucial for fine-tuning. We don't want to destroy the learned features.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --- Step 3: Replace the final classifier layer ---\n",
    "# EfficientNet's classifier is a nn.Sequential containing a Dropout and a Linear layer.\n",
    "# We need to find the number of input features to the final Linear layer.\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "\n",
    "# Now, we replace the original classifier with a new one for our specific number of classes.\n",
    "# It's good practice to include a Dropout layer for regularization.\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True), # Dropout with a probability of 0.3\n",
    "    nn.Linear(num_ftrs, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "# --- Step 4: Move the model to the correct device ---\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Model Summary for {MODEL_NAME} (before compilation) ---\")\n",
    "summary(model, input_size=(3, 224, 224))\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "# --- Step 3: Conditionally compile the model ---\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    # Check the operating system\n",
    "    if platform.system() == \"Windows\":\n",
    "        print(\"Windows OS detected. Skipping torch.compile() due to known issues with Triton dependency.\")\n",
    "    else:\n",
    "        # On Linux or MacOS, compile the model for a performance boost.\n",
    "        print(\"Linux or MacOS detected. Attempting to compile the model...\")\n",
    "        try:\n",
    "            model = torch.compile(model)\n",
    "            print(\"Model compiled successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Model compilation failed: {e}. Continuing without compilation.\")\n",
    "else:\n",
    "    print(\"PyTorch version is less than 2.0. Skipping torch.compile.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.148317,
     "end_time": "2020-12-15T06:54:33.881587",
     "exception": false,
     "start_time": "2020-12-15T06:54:33.733270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ—ï¸ Modelling ðŸ—ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.149596,
     "end_time": "2020-12-15T06:54:34.178417",
     "exception": false,
     "start_time": "2020-12-15T06:54:34.028821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It is advisable to use GPU instead of CPU when dealing with images dataset because CPUs are generalized for general purpose and GPUs are optimized for training deep learning models as they can process multiple computations simultaneously. They have a large number of cores, which allows for better computation of multiple parallel processes. Additionally, computations in deep learning need to handle huge amounts of data â€” this makes a GPUâ€™s memory bandwidth most suitable.\n",
    "To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 1 CONFIGURATION\n",
    "stage1_epochs = 5\n",
    "# Slightly reduced learning rate for more stable initial training of the head\n",
    "stage1_lr = 0.0005 \n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.classifier.parameters(), lr=stage1_lr, weight_decay=1e-4)\n",
    "\n",
    "# The scheduler is configured only for Stage 1\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=stage1_lr, \n",
    "                                          epochs=stage1_epochs, \n",
    "                                          steps_per_epoch=len(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_CHECKPOINT = True\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
    "start_epoch = 0\n",
    "if LOAD_CHECKPOINT and os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Checkpoint found at {CHECKPOINT_PATH}. Loading...\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming training from Epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10, start_epoch=0, early_stopping_patience=5, load_optimizer_state=True):\n",
    "    since = time.time()\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    best_epoch = start_epoch\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lrs': []}\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(device.type == 'cuda'))\n",
    "\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(f\"Checkpoint found at {CHECKPOINT_PATH}. Loading model weights...\")\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_acc = checkpoint.get('accuracy', 0.0)\n",
    "        print(f\"Loaded best accuracy from previous run: {best_acc:.4f}\")\n",
    "        \n",
    "        if LOAD_CHECKPOINT and load_optimizer_state:\n",
    "            print(\"Loading optimizer and scheduler state to resume training...\")\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming training from Epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(\"Starting a new training stage. Optimizer and scheduler are NOT loaded from checkpoint.\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}'); print('-' * 10)\n",
    "        for phase in ['train', 'val']:\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            running_loss, running_corrects = 0.0, 0\n",
    "            for inputs, labels in tqdm(dataloaders[phase], desc=f\"{phase} phase\"):\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    with torch.amp.autocast('cuda', enabled=(device.type == 'cuda')):\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    if phase == 'train':\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        # --- CHANGE IS HERE ---\n",
    "                        # Only step the scheduler per batch if it's NOT ReduceLROnPlateau\n",
    "                        if not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                            scheduler.step()\n",
    "                            history['lrs'].append(optimizer.param_groups[0]['lr'])\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss); history['train_acc'].append(epoch_acc.item())\n",
    "                writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "                writer.add_scalar('Accuracy/train', epoch_acc, epoch)\n",
    "            else: # val phase\n",
    "                history['val_loss'].append(epoch_loss); history['val_acc'].append(epoch_acc.item())\n",
    "                writer.add_scalar('Loss/val', epoch_loss, epoch)\n",
    "                writer.add_scalar('Accuracy/val', epoch_acc, epoch)\n",
    "                writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "                \n",
    "                # --- CHANGE IS HERE ---\n",
    "                # Step the ReduceLROnPlateau scheduler using the validation loss\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(epoch_loss)\n",
    "                    history['lrs'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc, best_epoch, epochs_no_improve = epoch_acc, epoch, 0\n",
    "                    best_model_wts = model.state_dict()\n",
    "                    save_path = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
    "                    torch.save({'epoch': epoch, 'model_state_dict': best_model_wts, 'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict(), 'loss': epoch_loss, 'accuracy': best_acc}, save_path)\n",
    "                    print(f\"New best model saved with accuracy: {best_acc:.4f}\")\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve >= early_stopping_patience:\n",
    "                        print(f\"Early stopping triggered. No improvement in {early_stopping_patience} validation epochs.\")\n",
    "                        model.load_state_dict(best_model_wts)\n",
    "                        writer.close()\n",
    "                        return model, history\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f} at epoch {best_epoch+1}')\n",
    "    last_epoch_path = os.path.join(CHECKPOINT_DIR, 'last_model.pth')\n",
    "    torch.save({\n",
    "        'epoch': num_epochs - 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "    }, last_epoch_path)\n",
    "    print(f\"Last checkpoint saved to {last_epoch_path}\")\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    writer.close()\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- STARTING STAGE 1: FEATURE EXTRACTION (TRAINING THE HEAD) ---\")\n",
    "model, history_stage1 = train_model(model, criterion, optimizer, scheduler, \n",
    "                                    num_epochs=stage1_epochs, \n",
    "                                    start_epoch=0, # Always start stage 1 from epoch 0\n",
    "                                    early_stopping_patience=3) # Early stopping for this stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Use this to find the optimal LR for Stage 2 ---\n",
    "print(\"--- Finding Optimal Learning Rate for Fine-Tuning ---\")\n",
    "\n",
    "# 1. Re-create the model and unfreeze all layers\n",
    "lr_finder_model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
    "for param in lr_finder_model.parameters():\n",
    "    param.requires_grad = True\n",
    "num_ftrs = lr_finder_model.classifier[1].in_features\n",
    "lr_finder_model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(num_ftrs, NUM_CLASSES)\n",
    ")\n",
    "lr_finder_model = lr_finder_model.to(device)\n",
    "\n",
    "# 2. Define a temporary optimizer and the same criterion\n",
    "temp_optimizer = optim.AdamW(lr_finder_model.parameters(), lr=1e-7, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# 3. Run the LRFinder\n",
    "lr_finder = LRFinder(lr_finder_model, temp_optimizer, criterion, device=device)\n",
    "lr_finder.range_test(dataloaders['train'], end_lr=1, num_iter=200)\n",
    "\n",
    "# 4. âœ¨ CAPTURE THE SUGGESTED LEARNING RATE AUTOMATICALLY âœ¨\n",
    "suggested_lr = lr_finder.suggestion()\n",
    "\n",
    "# 5. Plot the results and reset the model\n",
    "lr_finder.plot() \n",
    "lr_finder.reset() \n",
    "\n",
    "# 6. Print the captured value for confirmation\n",
    "print(f\"\\nLR Finder suggests a learning rate of: {suggested_lr:.2e}\")\n",
    "print(\"This value will now be used automatically for Stage 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_epochs = 40 \n",
    "# âœ¨ Using the optimal LR found automatically by the LR Finder âœ¨\n",
    "stage2_lr = suggested_lr   \n",
    "\n",
    "# --- Unfreeze all layers ---\n",
    "print(\"\\n--- UNFREEZING ALL LAYERS FOR FINE-TUNING ---\")\n",
    "# NOTE: We use the 'model' variable from Stage 1, not the 'lr_finder_model'\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# --- Create a new optimizer for the whole model ---\n",
    "optimizer = optim.AdamW(model.parameters(), lr=stage2_lr, weight_decay=1e-4)\n",
    "\n",
    "# --- Use the adaptive scheduler for fine-tuning ---\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                 mode='min',\n",
    "                                                 factor=0.2,\n",
    "                                                 patience=3,\n",
    "                                                 min_lr=1e-7)\n",
    "\n",
    "print(f\"Stage 2 configured with AdamW (LR={stage2_lr:.2e}) and ReduceLROnPlateau scheduler for {stage2_epochs} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- STARTING STAGE 2: FINE-TUNING (TRAINING ALL LAYERS) ---\")\n",
    "model, history_stage2 = train_model(model, criterion, optimizer, scheduler, \n",
    "                                    num_epochs=stage2_epochs, \n",
    "                                    start_epoch=0, # Start from epoch 0 for this new stage\n",
    "                                    early_stopping_patience=5,\n",
    "                                    load_optimizer_state=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- UNIFIED PLOTTING FUNCTION ---\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"\n",
    "    Plots training & validation accuracy/loss. Annotates best val accuracy.\n",
    "    \"\"\"\n",
    "    train_acc = history['train_acc']\n",
    "    val_acc = history['val_acc']\n",
    "    train_loss = history['train_loss']\n",
    "    val_loss = history['val_loss']\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))\n",
    "    fig.suptitle(f'Training History for {model_name}', fontsize=16)\n",
    "\n",
    "    ax1.plot(train_acc, label='Train Acc', marker='o')\n",
    "    ax1.plot(val_acc, label='Val Acc', marker='o')\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    if val_acc:\n",
    "        best_epoch = int(np.argmax(val_acc))\n",
    "        best_val_acc = val_acc[best_epoch]\n",
    "        ax1.axvline(best_epoch, color='r', linestyle='--', linewidth=1)\n",
    "        ax1.annotate(f'Best: {best_val_acc:.4f}\\nEp {best_epoch+1}',\n",
    "                     xy=(best_epoch, best_val_acc),\n",
    "                     xytext=(best_epoch, max(0, best_val_acc - 0.1)),\n",
    "                     arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                     ha='center', fontsize=10)\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(train_loss, label='Train Loss', marker='o')\n",
    "    ax2.plot(val_loss, label='Val Loss', marker='o')\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_learning_rate_schedule(history):\n",
    "    lrs = history.get('lrs', [])\n",
    "    if len(lrs) > 0:\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.plot(lrs)\n",
    "        plt.title('Learning Rate (per batch)')\n",
    "        plt.xlabel('Batch # (cumulative)')\n",
    "        plt.ylabel('LR')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No LR data to plot.')\n",
    "\n",
    "# --- Evaluation helpers ---\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc='Eval'):\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_labels, all_preds\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(labels, preds, class_names, normalize=True, max_classes_display=50):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    # If too many classes, show a subset heatmap legend only\n",
    "    if len(class_names) > max_classes_display:\n",
    "        print(f\"Too many classes ({len(class_names)}). Showing aggregated stats.\")\n",
    "        avg_diag = np.nanmean(np.diag(cm))\n",
    "        print(f\"Mean per-class accuracy: {avg_diag:.4f}\")\n",
    "        return\n",
    "    plt.figure(figsize=(min(1+0.3*len(class_names), 25), min(1+0.3*len(class_names), 25)))\n",
    "    sns.heatmap(cm, cmap='viridis', xticklabels=class_names, yticklabels=class_names, fmt='.2f' if normalize else 'd')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix' + (' (Normalized)' if normalize else ''))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_misclassified(model, dataloader, class_names, device, n=12):\n",
    "    model.eval()\n",
    "    images = []\n",
    "    labels_true = []\n",
    "    labels_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            mismatch = preds != labels.to(device)\n",
    "            if mismatch.any():\n",
    "                for i in torch.where(mismatch)[0]:\n",
    "                    images.append(inputs[i].cpu())\n",
    "                    labels_true.append(labels[i].item())\n",
    "                    labels_pred.append(preds[i].item())\n",
    "                    if len(images) >= n:\n",
    "                        break\n",
    "            if len(images) >= n:\n",
    "                break\n",
    "    if len(images) == 0:\n",
    "        print('No misclassifications found in this subset.')\n",
    "        return\n",
    "    # Denormalize for display\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    cols = 4\n",
    "    rows = int(np.ceil(len(images)/cols))\n",
    "    plt.figure(figsize=(cols*4, rows*4))\n",
    "    for idx, img in enumerate(images):\n",
    "        img_dn = img*std + mean\n",
    "        img_np = np.clip(img_dn.permute(1,2,0).numpy(), 0, 1)\n",
    "        ax = plt.subplot(rows, cols, idx+1)\n",
    "        ax.imshow(img_np)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"T: {class_names[labels_true[idx]][:15]}\\nP: {class_names[labels_pred[idx]][:15]}\", fontsize=9)\n",
    "    plt.suptitle('Sample Misclassified Images', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Grad-CAM (lightweight) ---\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        target_layer.register_forward_hook(self.forward_hook)\n",
    "        target_layer.register_backward_hook(self.backward_hook)\n",
    "\n",
    "    def forward_hook(self, module, inp, out):\n",
    "        self.activations = out.detach()\n",
    "\n",
    "    def backward_hook(self, module, grad_in, grad_out):\n",
    "        self.gradients = grad_out[0].detach()\n",
    "\n",
    "    def generate(self, input_tensor, class_idx=None):\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_tensor)\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1)\n",
    "        selected = output[0, class_idx]\n",
    "        selected.backward(retain_graph=True)\n",
    "        weights = self.gradients.mean(dim=[2,3], keepdim=True)\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        cam = torch.nn.functional.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
    "        cam_min, cam_max = cam.min(), cam.max()\n",
    "        cam = (cam - cam_min)/(cam_max - cam_min + 1e-8)\n",
    "        return cam[0,0].cpu().numpy()\n",
    "\n",
    "\n",
    "def visualize_gradcam(model, dataloader, class_names, device, n=3):\n",
    "    # Pick the last residual layer for Grad-CAM\n",
    "    target_layer = model.layer4[-1].bn2\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "\n",
    "    shown = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        for i in range(inputs.size(0)):\n",
    "            if shown >= n:\n",
    "                return\n",
    "            inp = inputs[i:i+1].to(device)\n",
    "            cam = gradcam.generate(inp)\n",
    "            img = (inputs[i]*std + mean).permute(1,2,0).numpy()\n",
    "            img = np.clip(img, 0, 1)\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(cam, cmap='jet', alpha=0.4)\n",
    "            pred_class = class_names[model(inp).argmax(1).item()]\n",
    "            plt.title(f\"True: {class_names[labels[i]]}\\nPred: {pred_class}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            shown += 1\n",
    "\n",
    "print(\"Visualization & evaluation helpers ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Generating Training Plots ---\")\n",
    "plot_training_history(history_stage2, MODEL_NAME)\n",
    "plot_learning_rate_schedule(history_stage2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "# IMPORTANT: Load the BEST model from checkpoint for final evaluation\n",
    "print(\"Loading best model for final evaluation...\")\n",
    "\n",
    "# Re-create the model architecture\n",
    "final_model = models.efficientnet_b3(weights=None) # Don't load pre-trained weights here\n",
    "num_ftrs = final_model.classifier[1].in_features\n",
    "final_model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(num_ftrs, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    # Check the operating system\n",
    "    if platform.system() != \"Windows\":\n",
    "        try:\n",
    "            final_model = torch.compile(final_model) # Compile the evaluation model too\n",
    "        except Exception as e:\n",
    "            print(f\"Could not compile the final model: {e}\")\n",
    "\n",
    "final_model.to(device)\n",
    "\n",
    "# Load the saved state dict of your best model\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "final_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Get predictions and labels\n",
    "test_labels, test_preds = evaluate_model(final_model, dataloaders['test'], class_names, device)\n",
    "\n",
    "# Print Classification Report\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(test_labels, test_preds, target_names=class_names, digits=4))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "plot_confusion_matrix(test_labels, test_preds, class_names)\n",
    "\n",
    "# Show some misclassified images\n",
    "print(\"\\n--- Sample Misclassified Images ---\")\n",
    "show_misclassified(final_model, dataloaders['test'], class_names, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "duration": 1424.697889,
   "end_time": "2020-12-15T07:16:24.493770",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-15T06:52:39.795881",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
